---
# NIMCache for downloading and caching the HuggingFace model
apiVersion: apps.nvidia.com/v1alpha1
kind: NIMCache
metadata:
  name: redhat-llama3-1b-fp8
  namespace: hacohen-nemo
spec:
  source:
    # For HuggingFace models, we use the huggingface source type
    huggingface:
      # The HuggingFace model repository
      modelName: "RedHatAI/Llama-3.2-1B-Instruct-FP8"
      # Reference to the HF token secret created in nemo_prerequisites.sh
      authSecret: hf-token
  storage:
    pvc:
      create: true
      storageClass: "gp3-csi"
      size: "50Gi"
      volumeAccessMode: ReadWriteOnce
  tolerations:
    - key: "g5-gpu"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"


---
# NIMPipeline to deploy the HuggingFace model with NeMo integration
apiVersion: apps.nvidia.com/v1alpha1
kind: NIMPipeline
metadata:
  name: llama-redhat-pipeline
  namespace: hacohen-nemo
spec:
  services:
    - name: redhat-llama3-1b-fp8
      enabled: true
      spec:
        env:
          # HuggingFace model configuration
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-token
                key: HF_TOKEN
          - name: NIM_MODEL_NAME
            value: "hf://RedHatAI/Llama-3.2-1B-Instruct-FP8"
          - name: NIM_SERVED_MODEL_NAME
            value: "redhat-llama-3.2-1b-fp8"

          # NeMo Integration (Critical for dynamic LoRA!)
          - name: NIM_PEFT_SOURCE
            value: http://nemoentitystore-sample.hacohen-nemo.svc.cluster.local:8000
          - name: NIM_PEFT_REFRESH_INTERVAL
            value: "180"
          - name: NIM_MAX_CPU_LORAS
            value: "16"
          - name: NIM_MAX_GPU_LORAS
            value: "8"
          - name: NIM_ENABLE_CUSTOMIZATION
            value: "true"
          - name: NIM_CUSTOMIZATION_ENABLED_MODELS
            value: "redhat-llama-3.2-1b-fp8"
          - name: NIM_GUIDED_DECODING_BACKEND
            value: lm-format-enforcer

        # Use the multi-LLM NIM container that supports HuggingFace models
        image:
          repository: nvcr.io/nim/nvidia/llm-nim
          tag: latest
          pullPolicy: Always
          pullSecrets:
          - ngc-secret

        authSecret: ngc-api-secret

        storage:
          nimCache:
            name: redhat-llama3-1b-fp8
            profile: ''

        replicas: 1

        resources:
          limits:
            nvidia.com/gpu: 1

        tolerations:
          - key: "g5-gpu"
            operator: "Equal"
            value: "true"
            effect: "NoSchedule"

        expose:
          service:
            type: ClusterIP
            port: 8000
