{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fda40cc-5239-4387-963e-c09af7ebd671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "from time import sleep, time\n",
    "from openai import OpenAI\n",
    "import asyncio, nest_asyncio\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724ffcd8-b75d-458f-9621-5e535bf1315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key for NVIDIA provider (required even for self-hosted services)\n",
    "os.environ[\"NVIDIA_API_KEY\"] = NDS_TOKEN\n",
    "\n",
    "# Metadata associated with Datasets and Customization Jobs\n",
    "os.environ[\"NVIDIA_DATASET_NAMESPACE\"] = NMS_NAMESPACE\n",
    "os.environ[\"NVIDIA_PROJECT_ID\"] = PROJECT_ID\n",
    "\n",
    "## Inference env vars\n",
    "os.environ[\"NVIDIA_BASE_URL\"] = NIM_URL\n",
    "\n",
    "# Data Store env vars\n",
    "os.environ[\"NVIDIA_DATASETS_URL\"] = ENTITY_STORE_URL\n",
    "\n",
    "## Customizer env vars\n",
    "os.environ[\"NVIDIA_CUSTOMIZER_URL\"] = CUSTOMIZER_URL\n",
    "os.environ[\"NVIDIA_OUTPUT_MODEL_DIR\"] = CUSTOMIZED_MODEL_DIR\n",
    "\n",
    "# Evaluator env vars\n",
    "os.environ[\"NVIDIA_EVALUATOR_URL\"] = EVALUATOR_URL\n",
    "\n",
    "# Guardrails env vars\n",
    "os.environ[\"GUARDRAILS_SERVICE_URL\"] = GUARDRAILS_URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeecaa8d-02b5-4fd4-9faa-6bf9c7daa830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Store endpoint: http://nemodatastore-sample.hacohen-nemo.svc.cluster.local:8000\n",
      "Entity Store endpoint: http://nemoentitystore-sample.hacohen-nemo.svc.cluster.local:8000\n",
      "Customizer endpoint: http://nemocustomizer-sample.hacohen-nemo.svc.cluster.local:8000\n",
      "Evaluator endpoint: http://nemoevaluator-sample.hacohen-nemo.svc.cluster.local:8000\n",
      "NIM endpoint: http://meta-llama3-1b-instruct.hacohen-nemo.svc.cluster.local:8000\n",
      "Namespace: xlam-tutorial-ns\n",
      "Base Model for Customization: meta/llama-3.2-1b-instruct\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Store endpoint: {DATA_STORE_URL}\")\n",
    "print(f\"Entity Store endpoint: {ENTITY_STORE_URL}\")\n",
    "print(f\"Customizer endpoint: {CUSTOMIZER_URL}\")\n",
    "print(f\"Evaluator endpoint: {EVALUATOR_URL}\")\n",
    "print(f\"NIM endpoint: {NIM_URL}\")\n",
    "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
    "print(f\"Base Model for Customization: {BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a67604-427b-4f41-ad63-3bc62c7dcad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OTEL_EXPORTER_OTLP_ENDPOINT is not set, skipping telemetry\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using config <span style=\"color: #000080; text-decoration-color: #000080\">nvidia</span>:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using config \u001b[34mnvidia\u001b[0m:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- files\n",
       "- inference\n",
       "- post_training\n",
       "- safety\n",
       "- scoring\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "container_image: null\n",
       "external_apis_dir: null\n",
       "external_providers_dir: null\n",
       "image_name: nvidia\n",
       "logging: null\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence:\n",
       "        agent_state:\n",
       "          backend: kv_default\n",
       "          namespace: agents\n",
       "        responses:\n",
       "          backend: sql_default\n",
       "          max_write_queue_size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span>\n",
       "          num_writers: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "          table_name: responses\n",
       "    module: null\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      dataset_namespace: xlam-tutorial-ns\n",
       "      datasets_url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://nemoentitystore-sample.hacohen-nemo.svc.cluster.local:8000</span>\n",
       "      project_id: test-project\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  eval:\n",
       "  - config:\n",
       "      evaluator_url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://nemoevaluator-sample.hacohen-nemo.svc.cluster.local:8000</span>\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  files:\n",
       "  - config:\n",
       "      metadata_store:\n",
       "        backend: sql_default\n",
       "        table_name: files_metadata\n",
       "      storage_dir: <span style=\"color: #800080; text-decoration-color: #800080\">/opt/app-root/src/.llama/distributions/nvidia/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">files</span>\n",
       "    module: null\n",
       "    provider_id: meta-reference-files\n",
       "    provider_type: inline::localfs\n",
       "  inference:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      append_api_version: true\n",
       "      url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://meta-llama3-1b-instruct.hacohen-nemo.svc.cluster.local:8000</span>\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  post_training:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      customizer_url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://nemocustomizer-sample.hacohen-nemo.svc.cluster.local:8000</span>\n",
       "      dataset_namespace: xlam-tutorial-ns\n",
       "      project_id: test-project\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  safety:\n",
       "  - config:\n",
       "      config_id: self-check\n",
       "      guardrails_service_url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://nemoguardrails-sample.hacohen-nemo.svc.cluster.local:8000</span>\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  scoring:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    module: null\n",
       "    provider_id: basic\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::ba</span>sic\n",
       "  tool_runtime:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    module: null\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  vector_io:\n",
       "  - config:\n",
       "      persistence:\n",
       "        backend: kv_default\n",
       "        namespace: vector_io::faiss\n",
       "    module: null\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::fa</span>iss\n",
       "registered_resources:\n",
       "  benchmarks: <span style=\"font-weight: bold\">[]</span>\n",
       "  datasets: <span style=\"font-weight: bold\">[]</span>\n",
       "  models: <span style=\"font-weight: bold\">[]</span>\n",
       "  scoring_fns: <span style=\"font-weight: bold\">[]</span>\n",
       "  shields: <span style=\"font-weight: bold\">[]</span>\n",
       "  tool_groups:\n",
       "  - args: null\n",
       "    mcp_endpoint: null\n",
       "    provider_id: rag-runtime\n",
       "    toolgroup_id: builtin::rag\n",
       "  vector_stores: <span style=\"font-weight: bold\">[]</span>\n",
       "server:\n",
       "  auth: null\n",
       "  cors: null\n",
       "  host: null\n",
       "  port: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8321</span>\n",
       "  quota: null\n",
       "  tls_cafile: null\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "  workers: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "storage:\n",
       "  backends:\n",
       "    kv_default:\n",
       "      db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/opt/app-root/src/.llama/distributions/nvidia/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">kvstore.db</span>\n",
       "      namespace: null\n",
       "      type: !!python/object/apply:llama_stack.core.storage.datatypes.StorageBackendType\n",
       "      - kv_sqlite\n",
       "    sql_default:\n",
       "      db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/opt/app-root/src/.llama/distributions/nvidia/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">sql_store.db</span>\n",
       "      type: !!python/object/apply:llama_stack.core.storage.datatypes.StorageBackendType\n",
       "      - sql_sqlite\n",
       "  stores:\n",
       "    conversations:\n",
       "      backend: sql_default\n",
       "      table_name: openai_conversations\n",
       "    inference:\n",
       "      backend: sql_default\n",
       "      max_write_queue_size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span>\n",
       "      num_writers: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "      table_name: inference_store\n",
       "    metadata:\n",
       "      backend: kv_default\n",
       "      namespace: registry\n",
       "    responses: null\n",
       "telemetry:\n",
       "  enabled: true\n",
       "vector_stores: null\n",
       "version: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- files\n",
       "- inference\n",
       "- post_training\n",
       "- safety\n",
       "- scoring\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "container_image: null\n",
       "external_apis_dir: null\n",
       "external_providers_dir: null\n",
       "image_name: nvidia\n",
       "logging: null\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence:\n",
       "        agent_state:\n",
       "          backend: kv_default\n",
       "          namespace: agents\n",
       "        responses:\n",
       "          backend: sql_default\n",
       "          max_write_queue_size: \u001b[1;36m10000\u001b[0m\n",
       "          num_writers: \u001b[1;36m4\u001b[0m\n",
       "          table_name: responses\n",
       "    module: null\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      dataset_namespace: xlam-tutorial-ns\n",
       "      datasets_url: \u001b[4;94mhttp://nemoentitystore-sample.hacohen-nemo.svc.cluster.local:8000\u001b[0m\n",
       "      project_id: test-project\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  eval:\n",
       "  - config:\n",
       "      evaluator_url: \u001b[4;94mhttp://nemoevaluator-sample.hacohen-nemo.svc.cluster.local:8000\u001b[0m\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  files:\n",
       "  - config:\n",
       "      metadata_store:\n",
       "        backend: sql_default\n",
       "        table_name: files_metadata\n",
       "      storage_dir: \u001b[35m/opt/app-root/src/.llama/distributions/nvidia/\u001b[0m\u001b[95mfiles\u001b[0m\n",
       "    module: null\n",
       "    provider_id: meta-reference-files\n",
       "    provider_type: inline::localfs\n",
       "  inference:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      append_api_version: true\n",
       "      url: \u001b[4;94mhttp://meta-llama3-1b-instruct.hacohen-nemo.svc.cluster.local:8000\u001b[0m\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  post_training:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      customizer_url: \u001b[4;94mhttp://nemocustomizer-sample.hacohen-nemo.svc.cluster.local:8000\u001b[0m\n",
       "      dataset_namespace: xlam-tutorial-ns\n",
       "      project_id: test-project\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  safety:\n",
       "  - config:\n",
       "      config_id: self-check\n",
       "      guardrails_service_url: \u001b[4;94mhttp://nemoguardrails-sample.hacohen-nemo.svc.cluster.local:8000\u001b[0m\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  scoring:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    module: null\n",
       "    provider_id: basic\n",
       "    provider_type: inlin\u001b[1;92me::ba\u001b[0msic\n",
       "  tool_runtime:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    module: null\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  vector_io:\n",
       "  - config:\n",
       "      persistence:\n",
       "        backend: kv_default\n",
       "        namespace: vector_io::faiss\n",
       "    module: null\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin\u001b[1;92me::fa\u001b[0miss\n",
       "registered_resources:\n",
       "  benchmarks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "  datasets: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "  models: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "  scoring_fns: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "  shields: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "  tool_groups:\n",
       "  - args: null\n",
       "    mcp_endpoint: null\n",
       "    provider_id: rag-runtime\n",
       "    toolgroup_id: builtin::rag\n",
       "  vector_stores: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "server:\n",
       "  auth: null\n",
       "  cors: null\n",
       "  host: null\n",
       "  port: \u001b[1;36m8321\u001b[0m\n",
       "  quota: null\n",
       "  tls_cafile: null\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "  workers: \u001b[1;36m1\u001b[0m\n",
       "storage:\n",
       "  backends:\n",
       "    kv_default:\n",
       "      db_path: \u001b[35m/opt/app-root/src/.llama/distributions/nvidia/\u001b[0m\u001b[95mkvstore.db\u001b[0m\n",
       "      namespace: null\n",
       "      type: !!python/object/apply:llama_stack.core.storage.datatypes.StorageBackendType\n",
       "      - kv_sqlite\n",
       "    sql_default:\n",
       "      db_path: \u001b[35m/opt/app-root/src/.llama/distributions/nvidia/\u001b[0m\u001b[95msql_store.db\u001b[0m\n",
       "      type: !!python/object/apply:llama_stack.core.storage.datatypes.StorageBackendType\n",
       "      - sql_sqlite\n",
       "  stores:\n",
       "    conversations:\n",
       "      backend: sql_default\n",
       "      table_name: openai_conversations\n",
       "    inference:\n",
       "      backend: sql_default\n",
       "      max_write_queue_size: \u001b[1;36m10000\u001b[0m\n",
       "      num_writers: \u001b[1;36m4\u001b[0m\n",
       "      table_name: inference_store\n",
       "    metadata:\n",
       "      backend: kv_default\n",
       "      namespace: registry\n",
       "    responses: null\n",
       "telemetry:\n",
       "  enabled: true\n",
       "vector_stores: null\n",
       "version: \u001b[1;36m2\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_stack.core.library_client import LlamaStackAsLibraryClient\n",
    "\n",
    "client = LlamaStackAsLibraryClient(\"nvidia\")\n",
    "client.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e9e75c3-bdb2-4300-a6ab-ef97fd87cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack.apis.common.job_types import JobStatus\n",
    "from llama_stack.core.datatypes import Api\n",
    "import asyncio\n",
    "\n",
    "def wait_customization_job(job_id: str, polling_interval: int = 30, timeout: int = 5500):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Access post_training through impls\n",
    "    post_training = client.async_client.impls[Api.post_training]\n",
    "    \n",
    "    # Get initial status using async\n",
    "    loop = asyncio.get_event_loop()\n",
    "    res = loop.run_until_complete(post_training.get_training_job_status(job_uuid=job_id))\n",
    "    job_status = res.status\n",
    "\n",
    "    print(f\"Waiting for Customization job {job_id} to finish.\")\n",
    "    print(f\"Job status: {job_status} after {time.time() - start_time} seconds.\")\n",
    "\n",
    "    while job_status in [JobStatus.scheduled.value, JobStatus.in_progress.value]:\n",
    "        sleep(polling_interval)\n",
    "        res = loop.run_until_complete(post_training.get_training_job_status(job_uuid=job_id))\n",
    "        job_status = res.status\n",
    "\n",
    "        print(f\"Job status: {job_status} after {time.time() - start_time} seconds.\")\n",
    "\n",
    "        if time.time() - start_time > timeout:\n",
    "            raise RuntimeError(f\"Customization Job {job_id} took more than {timeout} seconds.\")\n",
    "\n",
    "    return job_status\n",
    "\n",
    "\n",
    "# When creating a customized model, NIM asynchronously loads the model in its model registry.\n",
    "# After this, we can run inference with the new model. This helper function waits for NIM to pick up the new model.\n",
    "def wait_nim_loads_customized_model(model_id: str, polling_interval: int = 10, timeout: int = 300):\n",
    "    found = False\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Checking if NIM has loaded customized model {model_id}.\")\n",
    "\n",
    "    while not found:\n",
    "        sleep(polling_interval)\n",
    "\n",
    "        res = requests.get(f\"{NIM_URL}/v1/models\")\n",
    "        if model_id in [model[\"id\"] for model in res.json()[\"data\"]]:\n",
    "            found = True\n",
    "            print(f\"Model {model_id} available after {time.time() - start_time} seconds.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Model {model_id} not available after {time.time() - start_time} seconds.\")\n",
    "\n",
    "    if not found:\n",
    "        raise RuntimeError(f\"Model {model_id} not available after {timeout} seconds.\")\n",
    "\n",
    "    assert found, f\"Could not find model {model_id} in the list of available models.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed5a5d1a-eeda-4f9a-bd6a-977c07589306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlam-tutorial-ns/xlam-ft-dataset\n"
     ]
    }
   ],
   "source": [
    "repo_id = f\"{NMS_NAMESPACE}/{DATASET_NAME}\"\n",
    "print(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0fce0f-0f2a-4006-9611-99b9f665322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetRegisterResponse(identifier='xlam-ft-dataset', metadata={'format': 'json', 'description': 'Tool calling xLAM dataset in OpenAI ChatCompletions format', 'provider_id': 'nvidia'}, provider_id='nvidia', purpose='post-training/messages', source=SourceUriDataSource(type='uri', uri='hf://datasets/xlam-tutorial-ns/xlam-ft-dataset'), type='dataset', provider_resource_id='xlam-ft-dataset', owner=None)\n"
     ]
    }
   ],
   "source": [
    "response = client.datasets.register(\n",
    "    purpose=\"post-training/messages\",\n",
    "    dataset_id=DATASET_NAME,\n",
    "    source={\n",
    "        \"type\": \"uri\",\n",
    "        \"uri\": f\"hf://datasets/{repo_id}\"\n",
    "    },\n",
    "    metadata={\n",
    "        \"format\": \"json\",\n",
    "        \"description\": \"Tool calling xLAM dataset in OpenAI ChatCompletions format\",\n",
    "        \"provider_id\": \"nvidia\"\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "530839f6-d74e-4daa-a685-d3f87f93e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url=f\"{ENTITY_STORE_URL}/v1/datasets/{NMS_NAMESPACE}/{DATASET_NAME}\")\n",
    "assert res.status_code in (200, 201), f\"Status Code {res.status_code} Failed to fetch dataset {res.text}\"\n",
    "dataset_obj = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "462dd67f-a129-432e-940a-611bcb7daf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files URL: hf://datasets/xlam-tutorial-ns/xlam-ft-dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Files URL:\", dataset_obj[\"files_url\"])\n",
    "assert dataset_obj[\"files_url\"] == f\"hf://datasets/{repo_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "844c0966-64b1-4b56-a76d-21ab3fc3ded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraFinetuningConfig signature:\n",
      "(*, type: Literal['LoRA'] = 'LoRA', lora_attn_modules: list[str], apply_lora_to_mlp: bool, apply_lora_to_output: bool, rank: int, alpha: int, use_dora: bool | None = False, quantize_base: bool | None = False) -> None\n",
      "\n",
      "LoraFinetuningConfig fields:\n",
      "  type: required=False, default=LoRA\n",
      "  lora_attn_modules: required=True, default=PydanticUndefined\n",
      "  apply_lora_to_mlp: required=True, default=PydanticUndefined\n",
      "  apply_lora_to_output: required=True, default=PydanticUndefined\n",
      "  rank: required=True, default=PydanticUndefined\n",
      "  alpha: required=True, default=PydanticUndefined\n",
      "  use_dora: required=False, default=False\n",
      "  quantize_base: required=False, default=False\n"
     ]
    }
   ],
   "source": [
    "from llama_stack.apis.post_training import LoraFinetuningConfig\n",
    "import inspect\n",
    "\n",
    "print(\"LoraFinetuningConfig signature:\")\n",
    "print(inspect.signature(LoraFinetuningConfig))\n",
    "\n",
    "if hasattr(LoraFinetuningConfig, 'model_fields'):\n",
    "    print(\"\\nLoraFinetuningConfig fields:\")\n",
    "    for field_name, field_info in LoraFinetuningConfig.model_fields.items():\n",
    "        required = field_info.is_required()\n",
    "        print(f\"  {field_name}: required={required}, default={field_info.default}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21e03e04-4697-4999-97e8-b1a58917e4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/llama_stack/providers/remote/post_training/nvidia/post_training.py:337: UserWarning: Parameters: ['max_steps_per_epoch', 'gradient_accumulation_steps', 'max_validation_steps', 'efficiency_config', 'dtype'] in `TrainingConfig` not supported and will be ignored.\n",
      "  warn_unsupported_params(training_config, supported_params[\"training_config\"], \"TrainingConfig\")\n",
      "/opt/app-root/lib64/python3.12/site-packages/llama_stack/providers/remote/post_training/nvidia/post_training.py:338: UserWarning: Parameters: ['shuffle', 'data_format', 'validation_dataset_id', 'packed', 'train_on_input'] in `DataConfig` not supported and will be ignored.\n",
      "  warn_unsupported_params(training_config[\"data_config\"], supported_params[\"data_config\"], \"DataConfig\")\n",
      "/opt/app-root/lib64/python3.12/site-packages/llama_stack/providers/remote/post_training/nvidia/post_training.py:339: UserWarning: Parameters: ['optimizer_type', 'num_warmup_steps'] in `OptimizerConfig` not supported and will be ignored.\n",
      "  warn_unsupported_params(\n",
      "/opt/app-root/lib64/python3.12/site-packages/llama_stack/providers/remote/post_training/nvidia/post_training.py:393: UserWarning: Parameters: ['quantize_base', 'lora_attn_modules', 'apply_lora_to_output', 'apply_lora_to_mlp', 'use_dora', 'rank'] in `LoRA config` not supported and will be ignored.\n",
      "  warn_unsupported_params(algorithm_config, supported_params[\"lora_config\"], \"LoRA config\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_uuid='cust-A9bozbaPwTSBXzFCjYVs3' status=<JobStatus.in_progress: 'in_progress'> created_at=datetime.datetime(2025, 11, 6, 21, 58, 14, 363803) updated_at=datetime.datetime(2025, 11, 6, 21, 58, 14, 363806) id='cust-A9bozbaPwTSBXzFCjYVs3' namespace='default' project='test-project' dataset='xlam-tutorial-ns/xlam-ft-dataset' output_model='nvidia-tool-calling-tutorial/test-llama-stack@v1' config='meta/llama-3.2-1b-instruct@v1.0.0+A100' hyperparameters={'finetuning_type': 'lora', 'training_type': 'sft', 'batch_size': 16, 'epochs': 2, 'learning_rate': 0.0001, 'weight_decay': 0.01, 'lora': {'adapter_dim': 8, 'alpha': 16, 'adapter_dropout': None, 'target_modules': None}, 'sequence_packing_enabled': False} status_details={'created_at': '2025-11-06T21:58:15.222066', 'updated_at': '2025-11-06T21:58:15.222066', 'elapsed_time': 0.0, 'steps_completed': 0, 'epochs_completed': 0, 'percentage_done': 0.0, 'status_logs': [{'updated_at': '2025-11-06T21:58:15.222066', 'message': 'created'}]} config_snapshot={'base_model': 'meta/llama-3.2-1b-instruct', 'precision': 'bf16-mixed', 'training_option': {'training_type': 'sft', 'finetuning_type': 'lora', 'num_gpus': 1, 'num_nodes': 1, 'tensor_parallel_size': 1, 'data_parallel_size': 1, 'pipeline_parallel_size': 1, 'use_sequence_parallel': False, 'micro_batch_size': 1}, 'max_seq_length': 4096, 'prompt_template': '{prompt} {completion}'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from llama_stack.core.datatypes import Api\n",
    "from llama_stack.apis.post_training import (\n",
    "    TrainingConfig,\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    LoraFinetuningConfig,\n",
    "    DatasetFormat,\n",
    "    OptimizerType,\n",
    ")\n",
    "\n",
    "unique_suffix = int(time.time())\n",
    "\n",
    "# Access post_training through impls\n",
    "post_training = client.async_client.impls[Api.post_training]\n",
    "\n",
    "# Create proper config objects with all required fields\n",
    "data_config = DataConfig(\n",
    "    batch_size=16,\n",
    "    dataset_id=DATASET_NAME,\n",
    "    shuffle=True,\n",
    "    data_format=DatasetFormat.instruct\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig(\n",
    "    optimizer_type=OptimizerType.adamw,\n",
    "    lr=0.0001,\n",
    "    weight_decay=0.01,\n",
    "    num_warmup_steps=100\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    n_epochs=2,\n",
    "    data_config=data_config,\n",
    "    optimizer_config=optimizer_config\n",
    ")\n",
    "\n",
    "# LoRA configuration with correct fields\n",
    "algorithm_config = LoraFinetuningConfig(\n",
    "    lora_attn_modules=[],\n",
    "    apply_lora_to_mlp=True,\n",
    "    apply_lora_to_output=False,\n",
    "    rank=8,\n",
    "    alpha=16,\n",
    "    use_dora=False,\n",
    "    quantize_base=False\n",
    ")\n",
    "\n",
    "# Convert to dict to work around the bug\n",
    "training_config_dict = training_config.model_dump()\n",
    "\n",
    "# Now call the supervised_fine_tune method with dict\n",
    "res = await post_training.supervised_fine_tune(\n",
    "    job_uuid=f\"finetune-{unique_suffix}\",\n",
    "    model=\"meta/llama-3.2-1b-instruct@v1.0.0+A100\",\n",
    "    training_config=training_config_dict,  # Pass as dict\n",
    "    algorithm_config=algorithm_config,\n",
    "    hyperparam_search_config=None,\n",
    "    logger_config=None,\n",
    "    checkpoint_dir=\"\",\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eca4035-860a-45c0-9f17-f793e1d6b6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cust-A9bozbaPwTSBXzFCjYVs3\n",
      "nvidia-tool-calling-tutorial/test-llama-stack@v1\n"
     ]
    }
   ],
   "source": [
    "job = res.model_dump()\n",
    "\n",
    "# To job track status\n",
    "JOB_ID = job[\"id\"]\n",
    "\n",
    "# This will be the name of the model that will be used to send inference queries to\n",
    "CUSTOMIZED_MODEL = job[\"output_model\"]\n",
    "print(JOB_ID)\n",
    "print(CUSTOMIZED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9abaf4e4-339e-47d5-b1c6-71aed99dac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Customization job cust-A9bozbaPwTSBXzFCjYVs3 to finish.\n",
      "Job status: JobStatus.scheduled after 0.017461538314819336 seconds.\n"
     ]
    }
   ],
   "source": [
    "job_status = wait_customization_job(job_id=JOB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "138fcaa5-0c27-4cc2-bf1e-8eabb769c188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response JSON: {\n",
      "    \"object\": \"list\",\n",
      "    \"data\": [],\n",
      "    \"pagination\": {\n",
      "        \"page\": 1,\n",
      "        \"page_size\": 1000,\n",
      "        \"current_page_size\": 0,\n",
      "        \"total_pages\": 0,\n",
      "        \"total_results\": 0\n",
      "    },\n",
      "    \"sort\": \"-created_at\",\n",
      "    \"filter\": {\n",
      "        \"namespace\": \"xlam-tutorial-ns\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(f\"{ENTITY_STORE_URL}/v1/models\", params={\"filter[namespace]\": NMS_NAMESPACE, \"sort\" : \"-created_at\"})\n",
    "\n",
    "assert response.status_code == 200, f\"Status Code {response.status_code}: Request failed. Response: {response.text}\"\n",
    "print(\"Response JSON:\", json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11095c5e-e2ca-420b-af5c-f3cd398949de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if NIM has loaded customized model nvidia-tool-calling-tutorial/test-llama-stack@v1.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 10.008836269378662 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 20.014547109603882 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 30.019490718841553 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 40.024625301361084 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 50.029613733291626 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 60.03464388847351 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 70.04071593284607 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 80.04659008979797 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 90.051748752594 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 100.05656099319458 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 110.06210589408875 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 120.06673312187195 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 130.07179713249207 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 140.0795862674713 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 150.0846312046051 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 160.08975505828857 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 170.09637784957886 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 180.10159182548523 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 190.10662364959717 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 200.1125204563141 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 210.11760234832764 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 220.12275624275208 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 230.12864565849304 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 240.13355898857117 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 250.1387550830841 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 260.15117931365967 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 270.1564338207245 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 280.1614158153534 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 290.16757369041443 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 300.17271995544434 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 310.1786844730377 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 320.1848165988922 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 330.1898925304413 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 340.19559621810913 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 350.2010385990143 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 360.20599818229675 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 370.2124140262604 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 380.21871614456177 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 390.2237241268158 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 400.229820728302 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 410.23526096343994 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 420.2403426170349 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 430.2469274997711 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 440.25312876701355 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 450.2582275867462 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 460.26387190818787 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 470.2692892551422 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 480.2742908000946 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 490.28054022789 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 500.2868297100067 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 510.291743516922 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 520.2970237731934 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 530.3022198677063 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 540.3070776462555 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 550.3134701251984 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 560.3196077346802 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 570.3247683048248 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 580.3307075500488 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 590.3368647098541 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 600.3419008255005 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 610.3484587669373 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 620.3538765907288 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 630.358745098114 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 640.3646378517151 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 650.3711369037628 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 660.3763058185577 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 670.3828747272491 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 680.389586687088 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 690.394050359726 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 700.3988308906555 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 710.405855178833 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 720.4104657173157 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 730.4156374931335 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 740.4213082790375 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 750.4261798858643 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 760.4312002658844 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 770.437659740448 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 780.443244934082 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 790.4482662677765 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 800.4531009197235 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 810.4578959941864 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 820.4627866744995 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 830.4689772129059 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 840.4750969409943 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 850.4799439907074 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 860.484771490097 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 870.4902942180634 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 880.4956905841827 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 890.5002524852753 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 900.5062408447266 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 910.5110998153687 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 920.5156943798065 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 930.5213186740875 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 940.526526927948 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 950.5314753055573 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 960.5373311042786 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 970.5432684421539 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 980.5481915473938 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 990.5539495944977 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 1000.5589053630829 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 1010.5640382766724 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 not available after 1020.5695657730103 seconds.\n",
      "Model nvidia-tool-calling-tutorial/test-llama-stack@v1 available after 1030.5743799209595 seconds.\n"
     ]
    }
   ],
   "source": [
    "wait_nim_loads_customized_model(model_id=CUSTOMIZED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d608db8-4dd0-4a0c-be1a-f88de37359b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models in NIM:\n",
      "  - meta/llama-3.2-1b-instruct\n",
      "  - nvidia-tool-calling-tutorial/test-llama-stack@v1\n"
     ]
    }
   ],
   "source": [
    "# Verify the model is in NIM\n",
    "resp = requests.get(f\"{NIM_URL}/v1/models\")\n",
    "models = resp.json().get(\"data\", [])\n",
    "model_names = [model[\"id\"] for model in models]\n",
    "\n",
    "print(\"Available models in NIM:\")\n",
    "for name in model_names:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "assert CUSTOMIZED_MODEL in model_names, f\"Model {CUSTOMIZED_MODEL} not found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1de32b72-2dd3-4519-af47-03c8a4ab31f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models from NVIDIA provider:\n",
      "  - meta/llama-3.2-1b-instruct\n",
      "  - nvidia-tool-calling-tutorial/test-llama-stack@v1\n",
      "\n",
      "Routing table updated!\n"
     ]
    }
   ],
   "source": [
    "from llama_stack.core.datatypes import Api\n",
    "\n",
    "# Get the NVIDIA inference provider directly\n",
    "inference_router = client.async_client.impls[Api.inference]\n",
    "nvidia_provider = inference_router.routing_table.impls_by_provider_id.get(\"nvidia\")\n",
    "\n",
    "if nvidia_provider:\n",
    "    # Get fresh list of models from NVIDIA provider\n",
    "    models_from_provider = await nvidia_provider.list_models()\n",
    "    print(\"Models from NVIDIA provider:\")\n",
    "    for model in models_from_provider:\n",
    "        print(f\"  - {model.provider_resource_id}\")\n",
    "    \n",
    "    # Now update the routing table with these models\n",
    "    models_routing_table = client.async_client.impls[Api.models]\n",
    "    await models_routing_table.update_registered_models(\n",
    "        provider_id=\"nvidia\",\n",
    "        models=models_from_provider\n",
    "    )\n",
    "    print(\"\\nRouting table updated!\")\n",
    "    \n",
    "    # # Now try to register the model\n",
    "    # from llama_stack.apis.models import ModelType\n",
    "    # result = await models_routing_table.register_model(\n",
    "    #     model_id=CUSTOMIZED_MODEL,\n",
    "    #     model_type=ModelType.llm,\n",
    "    #     provider_id=\"nvidia\",\n",
    "    #     provider_model_id=CUSTOMIZED_MODEL,\n",
    "    # )\n",
    "    # print(\"\\nRegistration successful!\")\n",
    "    # print(result)\n",
    "else:\n",
    "    print(\"NVIDIA provider not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39217ee3-70cb-4124-b01b-65dfd600bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_stack.apis.models.models import ModelType\n",
    "\n",
    "# client.models.register(\n",
    "#     model_id=CUSTOMIZED_MODEL,\n",
    "#     model_type=ModelType.llm,\n",
    "#     provider_id=\"nvidia\",\n",
    "# )\n",
    "\n",
    "# Processed data will be stored here\n",
    "DATA_ROOT = os.path.join(os.getcwd(), \"sample_data\")\n",
    "CUSTOMIZATION_DATA_ROOT = os.path.join(DATA_ROOT, \"customization\")\n",
    "VALIDATION_DATA_ROOT = os.path.join(DATA_ROOT, \"validation\")\n",
    "EVALUATION_DATA_ROOT = os.path.join(DATA_ROOT, \"evaluation\")\n",
    "\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(CUSTOMIZATION_DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(VALIDATION_DATA_ROOT, exist_ok=True)\n",
    "os.makedirs(EVALUATION_DATA_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "081a209c-6480-4a4a-98e9-bb25a01b67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp = f\"{CUSTOMIZATION_DATA_ROOT}/training.jsonl\"\n",
    "assert os.path.exists(train_fp), f\"The training data at '{train_fp}' does not exist. Please ensure that the data was prepared successfully.\"\n",
    "\n",
    "val_fp = f\"{VALIDATION_DATA_ROOT}/validation.jsonl\"\n",
    "assert os.path.exists(val_fp), f\"The validation data at '{val_fp}' does not exist. Please ensure that the data was prepared successfully.\"\n",
    "\n",
    "test_fp = f\"{EVALUATION_DATA_ROOT}/xlam-test-single.jsonl\"\n",
    "assert os.path.exists(test_fp), f\"The test data at '{test_fp}' does not exist. Please ensure that the data was prepared successfully.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2f3d146-1774-4a1e-9542-81adadca8cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 713 examples in the test set\n"
     ]
    }
   ],
   "source": [
    "def read_jsonl(file_path):\n",
    "    \"\"\"Reads a JSON Lines file and yields parsed JSON objects\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            if not line:\n",
    "                continue  # Skip empty lines\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "test_data = list(read_jsonl(test_fp))\n",
    "\n",
    "print(f\"There are {len(test_data)} examples in the test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e32dfb4-e2e8-4fe2-8453-c7696338f441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tool_name': 'getpeople',\n",
       "  'description': 'Fetches a list of artificial intelligence influencers, entrepreneurs, and top researchers from the specified API endpoint.',\n",
       "  'parameters': {'page': {'param_type': 'integer',\n",
       "    'description': 'The page number to retrieve.',\n",
       "    'default': '1'}}},\n",
       " {'tool_name': 'movies_get_opening',\n",
       "  'description': 'Fetches the list of opening movies for a given country using the Flixster API.',\n",
       "  'parameters': {'countryid': {'param_type': 'string',\n",
       "    'description': \"The country code for which to fetch opening movies. Defaults to 'usa'. Examples include 'afg', 'alb', 'dza', etc.\",\n",
       "    'default': 'usa'}}},\n",
       " {'tool_name': 'overview',\n",
       "  'description': 'Fetches summary statistics from the Papercliff API regarding the number of keywords found, and the number of articles and agencies reviewed.',\n",
       "  'parameters': {'is_from': {'param_type': 'string',\n",
       "    'description': \"Narrows down the results to articles published after the provided date-time. The format should be `yyyy-MM-dd'T'HH:mm`. Example value: `2022-09-18T13:45`. Date-times older than a week do not affect the result.\"},\n",
       "   'to': {'param_type': 'string',\n",
       "    'description': \"Narrows down the results to articles published before the provided date-time. The format should be `yyyy-MM-dd'T'HH:mm`. Example value: `2022-09-18T15:30`. Date-times in the future do not affect the result.\"},\n",
       "   'terms': {'param_type': 'string',\n",
       "    'description': 'Narrows down the results to articles that contain all the provided keywords. The terms should consist of one to three words separated by a dash. Example value: `election-campaign`.'}}},\n",
       " {'tool_name': 'title_get_parental_guide',\n",
       "  'description': 'Fetches parental guide information for a specific movie from the online movie database.',\n",
       "  'parameters': {'tconst': {'param_type': 'string',\n",
       "    'description': \"The unique identifier for the movie. It should start with 'tt' and can be obtained from the `/title/auto-complete` or `/title/find` endpoints. For example, 'tt0944947'.\",\n",
       "    'default': 'tt0944947'}}},\n",
       " {'tool_name': 'get_individual_news_sources',\n",
       "  'description': 'Fetches cryptocurrency news from a specific news source using the provided API key.',\n",
       "  'parameters': {'newspaperid': {'param_type': 'string',\n",
       "    'description': 'The ID of the specific news source to fetch news from.',\n",
       "    'default': 'CryptoNews'}}},\n",
       " {'tool_name': 'get_specific_climate_change_news',\n",
       "  'description': 'Fetch specific climate change news from a given newspaper.',\n",
       "  'parameters': {'newspaperid': {'param_type': 'string',\n",
       "    'description': 'The ID of the newspaper from which to fetch climate change news.',\n",
       "    'default': 'guardian'}}},\n",
       " {'tool_name': 'movies_get_upcoming',\n",
       "  'description': 'Fetches a list of upcoming movies based on the specified parameters.',\n",
       "  'parameters': {'limit': {'param_type': 'integer',\n",
       "    'description': 'The number of items to return, with a maximum of 100. Defaults to 100.',\n",
       "    'default': '100'},\n",
       "   'countryid': {'param_type': 'string',\n",
       "    'description': \"The country identifier to filter the upcoming movies list. Defaults to 'usa'. Example values include 'afg', 'alb', 'dza', etc.\",\n",
       "    'default': 'usa'}}}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Randomly choose\n",
    "test_sample = random.choice(test_data)\n",
    "\n",
    "# Transform tools to format expected by Llama Stack client\n",
    "for i, tool in enumerate(test_sample['tools']):\n",
    "    # Extract properties we will map to the expected format\n",
    "    tool = tool.get('function', {})\n",
    "    tool_name = tool.get('name')\n",
    "    tool_description = tool.get('description')\n",
    "    tool_params = tool.get('parameters', {})\n",
    "    tool_params_properties = tool_params.get('properties', {})\n",
    "\n",
    "    # Create object of parameters for this tool\n",
    "    transformed_parameters = {}\n",
    "    for name, property in tool_params_properties.items():\n",
    "        transformed_param = {\n",
    "            'param_type': property.get('type'),\n",
    "            'description': property.get('description')\n",
    "        }\n",
    "        if 'default' in property:\n",
    "            transformed_param['default'] = property['default']\n",
    "        if 'required' in property:\n",
    "            transformed_param['required'] = property['required']\n",
    "\n",
    "        transformed_parameters[name] = transformed_param\n",
    "\n",
    "    # Update this tool in-place using the expected format\n",
    "    test_sample['tools'][i] = {\n",
    "        'tool_name': tool_name,\n",
    "        'description': tool_description,\n",
    "        'parameters': transformed_parameters\n",
    "    }\n",
    "\n",
    "# Visualize the inputs to the LLM - user query and available tools\n",
    "test_sample['messages']\n",
    "test_sample['tools']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "044ff4b7-c526-4bcc-87b9-639e1ccba9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI formatted tools:\n",
      "{'type': 'function', 'function': {'name': 'getpeople', 'description': 'Fetches a list of artificial intelligence influencers, entrepreneurs, and top researchers from the specified API endpoint.', 'parameters': {'type': 'object', 'properties': {'page': {'type': 'integer', 'description': 'The page number to retrieve.', 'default': '1'}}, 'required': []}}}\n"
     ]
    }
   ],
   "source": [
    "from llama_stack.core.datatypes import Api\n",
    "\n",
    "# Use the registered model ID\n",
    "REGISTERED_MODEL_ID = \"nvidia/nvidia-tool-calling-tutorial/test-llama-stack@v1\"\n",
    "\n",
    "# Transform tools back to OpenAI format\n",
    "openai_tools = []\n",
    "for tool in test_sample['tools']:\n",
    "    # Check if it's already in OpenAI format (has 'function' key)\n",
    "    if 'function' in tool:\n",
    "        openai_tools.append(tool)\n",
    "    else:\n",
    "        # Convert from Llama Stack format to OpenAI format\n",
    "        openai_tool = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.get('tool_name'),\n",
    "                \"description\": tool.get('description'),\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {},\n",
    "                    \"required\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        # Convert parameters\n",
    "        for param_name, param_info in tool.get('parameters', {}).items():\n",
    "            openai_tool[\"function\"][\"parameters\"][\"properties\"][param_name] = {\n",
    "                \"type\": param_info.get('param_type'),\n",
    "                \"description\": param_info.get('description', '')\n",
    "            }\n",
    "            if param_info.get('default') is not None:\n",
    "                openai_tool[\"function\"][\"parameters\"][\"properties\"][param_name][\"default\"] = param_info['default']\n",
    "            if param_info.get('required', False):\n",
    "                openai_tool[\"function\"][\"parameters\"][\"required\"].append(param_name)\n",
    "        \n",
    "        openai_tools.append(openai_tool)\n",
    "\n",
    "print(\"OpenAI formatted tools:\")\n",
    "print(openai_tools[0] if openai_tools else \"No tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb9e61d1-e731-42d2-a8ac-709fc1345d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calls from model:\n",
      "[ChatCompletionMessageFunctionToolCall(id='chatcmpl-tool-13119f0fdcf649e0bf0a73499a42982a', function=Function(arguments='{\"page\": 2}', name='getpeople'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "from llama_stack.core.datatypes import Api\n",
    "from llama_stack.apis.inference import OpenAIChatCompletionRequestWithExtraBody\n",
    "\n",
    "# Access inference through impls\n",
    "inference = client.async_client.impls[Api.inference]\n",
    "\n",
    "# Create request with OpenAI-formatted tools\n",
    "request = OpenAIChatCompletionRequestWithExtraBody(\n",
    "    model=REGISTERED_MODEL_ID,\n",
    "    messages=test_sample[\"messages\"],\n",
    "    tools=openai_tools,  # Use the converted tools\n",
    "    tool_choice=\"auto\",\n",
    "    stream=False,\n",
    "    max_tokens=512,\n",
    "    temperature=0.1,\n",
    "    top_p=0.7,\n",
    ")\n",
    "\n",
    "# Make the chat completion call\n",
    "completion = await inference.openai_chat_completion(params=request)\n",
    "\n",
    "print(\"Tool calls from model:\")\n",
    "if hasattr(completion, 'choices') and len(completion.choices) > 0:\n",
    "    print(completion.choices[0].message.tool_calls)\n",
    "else:\n",
    "    print(completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94c10a4d-3589-42d2-a3aa-f5a35aae739e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'getpeople', 'arguments': {'page': 2}}}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample['tool_calls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11c5455c-e690-4e8d-9be4-35f9118baf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of your custom model is: nvidia-tool-calling-tutorial/test-llama-stack@v1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Name of your custom model is: {CUSTOMIZED_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a83600-9438-4029-b95d-81ed10c82f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
