{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6097a43a-7a49-4c0b-8289-0057a609d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import random\n",
    "from time import sleep, time\n",
    "from openai import OpenAI\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f13eb21-8975-4f78-bede-4d0e0123959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for LlamaStack\n",
    "os.environ[\"NVIDIA_DATASET_NAMESPACE\"] = NMS_NAMESPACE\n",
    "os.environ[\"NVIDIA_PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"NVIDIA_BASE_URL\"] = NIM_URL\n",
    "os.environ[\"NVIDIA_DATASETS_URL\"] = ENTITY_STORE_URL\n",
    "os.environ[\"NVIDIA_CUSTOMIZER_URL\"] = CUSTOMIZER_URL\n",
    "os.environ[\"NVIDIA_OUTPUT_MODEL_DIR\"] = CUSTOMIZED_MODEL_DIR\n",
    "os.environ[\"NVIDIA_EVALUATOR_URL\"] = EVALUATOR_URL\n",
    "os.environ[\"GUARDRAILS_SERVICE_URL\"] = GUARDRAILS_URL\n",
    "os.environ[\"NVIDIA_GUARDRAILS_CONFIG_ID\"] = \"demo-self-check-input-output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9c1d1a-5ed6-4992-996c-2a9ca6c41943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OTEL_EXPORTER_OTLP_ENDPOINT is not set, skipping telemetry\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using config <span style=\"color: #000080; text-decoration-color: #000080\">nvidia</span>:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using config \u001b[34mnvidia\u001b[0m:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- files\n",
       "- inference\n",
       "- post_training\n",
       "- safety\n",
       "- scoring\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "container_image: null\n",
       "external_apis_dir: null\n",
       "external_providers_dir: null\n",
       "image_name: nvidia\n",
       "logging: null\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence:\n",
       "        agent_state:\n",
       "          backend: kv_default\n",
       "          namespace: agents\n",
       "        responses:\n",
       "          backend: sql_default\n",
       "          max_write_queue_size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span>\n",
       "          num_writers: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "          table_name: responses\n",
       "    module: null\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      dataset_namespace: xlam-tutorial-ns\n",
       "      datasets_url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://nemoentitystore-sample.hacohen-nemo.svc.cluster.local:8000</span>\n",
       "      project_id: test-project\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  eval:\n",
       "  - config:\n",
       "      evaluator_url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://nemoevaluator-sample.hacohen-nemo.svc.cluster.local:8000</span>\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  files:\n",
       "  - config:\n",
       "      metadata_store:\n",
       "        backend: sql_default\n",
       "        table_name: files_metadata\n",
       "      storage_dir: <span style=\"color: #800080; text-decoration-color: #800080\">/opt/app-root/src/.llama/distributions/nvidia/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">files</span>\n",
       "    module: null\n",
       "    provider_id: meta-reference-files\n",
       "    provider_type: inline::localfs\n",
       "  inference:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      append_api_version: true\n",
       "      url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://meta-llama3-1b-instruct.hacohen-nemo.svc.cluster.local:8000</span>\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  post_training:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      customizer_url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://nemocustomizer-sample.hacohen-nemo.svc.cluster.local:8000</span>\n",
       "      dataset_namespace: xlam-tutorial-ns\n",
       "      project_id: test-project\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  safety:\n",
       "  - config:\n",
       "      config_id: demo-self-check-input-output\n",
       "      guardrails_service_url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://nemoguardrails-sample.hacohen-nemo.svc.cluster.local:8000</span>\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  scoring:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    module: null\n",
       "    provider_id: basic\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::ba</span>sic\n",
       "  tool_runtime:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    module: null\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  vector_io:\n",
       "  - config:\n",
       "      persistence:\n",
       "        backend: kv_default\n",
       "        namespace: vector_io::faiss\n",
       "    module: null\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::fa</span>iss\n",
       "registered_resources:\n",
       "  benchmarks: <span style=\"font-weight: bold\">[]</span>\n",
       "  datasets: <span style=\"font-weight: bold\">[]</span>\n",
       "  models: <span style=\"font-weight: bold\">[]</span>\n",
       "  scoring_fns: <span style=\"font-weight: bold\">[]</span>\n",
       "  shields: <span style=\"font-weight: bold\">[]</span>\n",
       "  tool_groups:\n",
       "  - args: null\n",
       "    mcp_endpoint: null\n",
       "    provider_id: rag-runtime\n",
       "    toolgroup_id: builtin::rag\n",
       "  vector_stores: <span style=\"font-weight: bold\">[]</span>\n",
       "server:\n",
       "  auth: null\n",
       "  cors: null\n",
       "  host: null\n",
       "  port: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8321</span>\n",
       "  quota: null\n",
       "  tls_cafile: null\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "  workers: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "storage:\n",
       "  backends:\n",
       "    kv_default:\n",
       "      db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/opt/app-root/src/.llama/distributions/nvidia/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">kvstore.db</span>\n",
       "      namespace: null\n",
       "      type: !!python/object/apply:llama_stack.core.storage.datatypes.StorageBackendType\n",
       "      - kv_sqlite\n",
       "    sql_default:\n",
       "      db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/opt/app-root/src/.llama/distributions/nvidia/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">sql_store.db</span>\n",
       "      type: !!python/object/apply:llama_stack.core.storage.datatypes.StorageBackendType\n",
       "      - sql_sqlite\n",
       "  stores:\n",
       "    conversations:\n",
       "      backend: sql_default\n",
       "      table_name: openai_conversations\n",
       "    inference:\n",
       "      backend: sql_default\n",
       "      max_write_queue_size: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span>\n",
       "      num_writers: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>\n",
       "      table_name: inference_store\n",
       "    metadata:\n",
       "      backend: kv_default\n",
       "      namespace: registry\n",
       "    responses: null\n",
       "telemetry:\n",
       "  enabled: true\n",
       "vector_stores: null\n",
       "version: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- files\n",
       "- inference\n",
       "- post_training\n",
       "- safety\n",
       "- scoring\n",
       "- tool_runtime\n",
       "- vector_io\n",
       "container_image: null\n",
       "external_apis_dir: null\n",
       "external_providers_dir: null\n",
       "image_name: nvidia\n",
       "logging: null\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence:\n",
       "        agent_state:\n",
       "          backend: kv_default\n",
       "          namespace: agents\n",
       "        responses:\n",
       "          backend: sql_default\n",
       "          max_write_queue_size: \u001b[1;36m10000\u001b[0m\n",
       "          num_writers: \u001b[1;36m4\u001b[0m\n",
       "          table_name: responses\n",
       "    module: null\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      dataset_namespace: xlam-tutorial-ns\n",
       "      datasets_url: \u001b[4;94mhttp://nemoentitystore-sample.hacohen-nemo.svc.cluster.local:8000\u001b[0m\n",
       "      project_id: test-project\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  eval:\n",
       "  - config:\n",
       "      evaluator_url: \u001b[4;94mhttp://nemoevaluator-sample.hacohen-nemo.svc.cluster.local:8000\u001b[0m\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  files:\n",
       "  - config:\n",
       "      metadata_store:\n",
       "        backend: sql_default\n",
       "        table_name: files_metadata\n",
       "      storage_dir: \u001b[35m/opt/app-root/src/.llama/distributions/nvidia/\u001b[0m\u001b[95mfiles\u001b[0m\n",
       "    module: null\n",
       "    provider_id: meta-reference-files\n",
       "    provider_type: inline::localfs\n",
       "  inference:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      append_api_version: true\n",
       "      url: \u001b[4;94mhttp://meta-llama3-1b-instruct.hacohen-nemo.svc.cluster.local:8000\u001b[0m\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  post_training:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      customizer_url: \u001b[4;94mhttp://nemocustomizer-sample.hacohen-nemo.svc.cluster.local:8000\u001b[0m\n",
       "      dataset_namespace: xlam-tutorial-ns\n",
       "      project_id: test-project\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  safety:\n",
       "  - config:\n",
       "      config_id: demo-self-check-input-output\n",
       "      guardrails_service_url: \u001b[4;94mhttp://nemoguardrails-sample.hacohen-nemo.svc.cluster.local:8000\u001b[0m\n",
       "    module: null\n",
       "    provider_id: nvidia\n",
       "    provider_type: remote::nvidia\n",
       "  scoring:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    module: null\n",
       "    provider_id: basic\n",
       "    provider_type: inlin\u001b[1;92me::ba\u001b[0msic\n",
       "  tool_runtime:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    module: null\n",
       "    provider_id: rag-runtime\n",
       "    provider_type: inline::rag-runtime\n",
       "  vector_io:\n",
       "  - config:\n",
       "      persistence:\n",
       "        backend: kv_default\n",
       "        namespace: vector_io::faiss\n",
       "    module: null\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin\u001b[1;92me::fa\u001b[0miss\n",
       "registered_resources:\n",
       "  benchmarks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "  datasets: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "  models: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "  scoring_fns: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "  shields: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "  tool_groups:\n",
       "  - args: null\n",
       "    mcp_endpoint: null\n",
       "    provider_id: rag-runtime\n",
       "    toolgroup_id: builtin::rag\n",
       "  vector_stores: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "server:\n",
       "  auth: null\n",
       "  cors: null\n",
       "  host: null\n",
       "  port: \u001b[1;36m8321\u001b[0m\n",
       "  quota: null\n",
       "  tls_cafile: null\n",
       "  tls_certfile: null\n",
       "  tls_keyfile: null\n",
       "  workers: \u001b[1;36m1\u001b[0m\n",
       "storage:\n",
       "  backends:\n",
       "    kv_default:\n",
       "      db_path: \u001b[35m/opt/app-root/src/.llama/distributions/nvidia/\u001b[0m\u001b[95mkvstore.db\u001b[0m\n",
       "      namespace: null\n",
       "      type: !!python/object/apply:llama_stack.core.storage.datatypes.StorageBackendType\n",
       "      - kv_sqlite\n",
       "    sql_default:\n",
       "      db_path: \u001b[35m/opt/app-root/src/.llama/distributions/nvidia/\u001b[0m\u001b[95msql_store.db\u001b[0m\n",
       "      type: !!python/object/apply:llama_stack.core.storage.datatypes.StorageBackendType\n",
       "      - sql_sqlite\n",
       "  stores:\n",
       "    conversations:\n",
       "      backend: sql_default\n",
       "      table_name: openai_conversations\n",
       "    inference:\n",
       "      backend: sql_default\n",
       "      max_write_queue_size: \u001b[1;36m10000\u001b[0m\n",
       "      num_writers: \u001b[1;36m4\u001b[0m\n",
       "      table_name: inference_store\n",
       "    metadata:\n",
       "      backend: kv_default\n",
       "      namespace: registry\n",
       "    responses: null\n",
       "telemetry:\n",
       "  enabled: true\n",
       "vector_stores: null\n",
       "version: \u001b[1;36m2\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_stack.core.library_client import LlamaStackAsLibraryClient\n",
    "\n",
    "client = LlamaStackAsLibraryClient(\"nvidia\")\n",
    "client.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88bebf53-7279-4dd2-a4d0-fccce2e52998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Store endpoint: http://nemodatastore-sample.hacohen-nemo.svc.cluster.local:8000\n",
      "Entity Store endpoint: http://nemoentitystore-sample.hacohen-nemo.svc.cluster.local:8000\n",
      "Customizer endpoint: http://nemocustomizer-sample.hacohen-nemo.svc.cluster.local:8000\n",
      "Evaluator endpoint: http://nemoevaluator-sample.hacohen-nemo.svc.cluster.local:8000\n",
      "Guardrails endpoint: http://nemoguardrails-sample.hacohen-nemo.svc.cluster.local:8000\n",
      "NIM endpoint: http://meta-llama3-1b-instruct.hacohen-nemo.svc.cluster.local:8000\n",
      "Namespace: xlam-tutorial-ns\n",
      "Base Model: meta/llama-3.2-1b-instruct\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Store endpoint: {NDS_URL}\")\n",
    "print(f\"Entity Store endpoint: {ENTITY_STORE_URL}\")\n",
    "print(f\"Customizer endpoint: {CUSTOMIZER_URL}\")\n",
    "print(f\"Evaluator endpoint: {EVALUATOR_URL}\")\n",
    "print(f\"Guardrails endpoint: {GUARDRAILS_URL}\")\n",
    "print(f\"NIM endpoint: {NIM_URL}\")\n",
    "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
    "print(f\"Base Model: {BASE_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3db84c-a5d7-45a9-b3dc-2e047c43a089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Simplified guardrails configuration created\n",
      "{\n",
      "  \"created_at\": \"2025-11-07T17:48:03.689954\",\n",
      "  \"updated_at\": \"2025-11-07T17:48:03.689957\",\n",
      "  \"name\": \"demo-self-check-input-output\",\n",
      "  \"namespace\": \"default\",\n",
      "  \"description\": \"Simple content moderation without self-check flows\",\n",
      "  \"data\": {\n",
      "    \"models\": [\n",
      "      {\n",
      "        \"type\": \"main\",\n",
      "        \"engine\": \"openai\",\n",
      "        \"model\": \"meta/llama-3.2-1b-instruct\",\n",
      "        \"api_key_env_var\": null,\n",
      "        \"reasoning_config\": {\n",
      "          \"remove_reasoning_traces\": true,\n",
      "          \"remove_thinking_traces\": null,\n",
      "          \"start_token\": \"<think>\",\n",
      "          \"end_token\": \"</think>\"\n",
      "        },\n",
      "        \"parameters\": {},\n",
      "        \"mode\": \"chat\"\n",
      "      }\n",
      "    ],\n",
      "    \"instructions\": [\n",
      "      {\n",
      "        \"type\": \"general\",\n",
      "        \"content\": \"Below is a conversation between a helpful AI assistant and a user. The bot is designed to generate human-like text based on the input that it receives. The bot is talkative and provides lots of specific details. If the bot does not know the answer to a question, it truthfully says it does not know.\"\n",
      "      }\n",
      "    ],\n",
      "    \"actions_server_url\": null,\n",
      "    \"sample_conversation\": \"user \\\"Hello there!\\\"\\n  express greeting\\nbot express greeting\\n  \\\"Hello! How can I assist you today?\\\"\\nuser \\\"What can you do for me?\\\"\\n  ask about capabilities\\nbot respond about capabilities\\n  \\\"As an AI assistant, I can help you with a wide range of tasks. This includes question answering on various topics, generating text for various purposes and providing suggestions based on your preferences.\\\"\\nuser \\\"Tell me a bit about the history of NVIDIA.\\\"\\n  ask general question\\nbot response for general question\\n  \\\"NVIDIA is a technology company that specializes in designing and manufacturing graphics processing units (GPUs) and other computer hardware. The company was founded in 1993 by Jen-Hsun Huang, Chris Malachowsky, and Curtis Priem.\\\"\\nuser \\\"tell me more\\\"\\n  request more information\\nbot provide more information\\n  \\\"Initially, the company focused on developing 3D graphics processing technology for the PC gaming market. In 1999, NVIDIA released the GeForce 256, the world's first GPU, which was a major breakthrough for the gaming industry. The company continued to innovate in the GPU space, releasing new products and expanding into other markets such as professional graphics, mobile devices, and artificial intelligence.\\\"\\nuser \\\"thanks\\\"\\n  express appreciation\\nbot express appreciation and offer additional help\\n  \\\"You're welcome. If you have any more questions or if there's anything else I can help you with, please don't hesitate to ask.\\\"\\n\",\n",
      "    \"prompts\": null,\n",
      "    \"prompting_mode\": \"standard\",\n",
      "    \"lowest_temperature\": 0.001,\n",
      "    \"enable_multi_step_generation\": false,\n",
      "    \"colang_version\": \"1.0\",\n",
      "    \"custom_data\": {},\n",
      "    \"rails\": {\n",
      "      \"config\": null,\n",
      "      \"input\": {\n",
      "        \"flows\": []\n",
      "      },\n",
      "      \"output\": {\n",
      "        \"flows\": [],\n",
      "        \"streaming\": {\n",
      "          \"enabled\": false,\n",
      "          \"chunk_size\": 200,\n",
      "          \"context_size\": 50,\n",
      "          \"stream_first\": true\n",
      "        },\n",
      "        \"apply_to_reasoning_traces\": false\n",
      "      },\n",
      "      \"retrieval\": {\n",
      "        \"flows\": []\n",
      "      },\n",
      "      \"dialog\": {\n",
      "        \"single_call\": {\n",
      "          \"enabled\": false,\n",
      "          \"fallback_to_multiple_calls\": true\n",
      "        },\n",
      "        \"user_messages\": {\n",
      "          \"embeddings_only\": false,\n",
      "          \"embeddings_only_similarity_threshold\": null,\n",
      "          \"embeddings_only_fallback_intent\": null\n",
      "        }\n",
      "      },\n",
      "      \"actions\": {\n",
      "        \"instant_actions\": null\n",
      "      }\n",
      "    },\n",
      "    \"enable_rails_exceptions\": false,\n",
      "    \"passthrough\": null\n",
      "  },\n",
      "  \"files_url\": null,\n",
      "  \"schema_version\": \"1.0\",\n",
      "  \"project\": null,\n",
      "  \"custom_fields\": {},\n",
      "  \"ownership\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "simple_working_config = {\n",
    "    \"name\": \"demo-self-check-input-output\",\n",
    "    \"namespace\": \"default\",\n",
    "    \"description\": \"Simple content moderation without self-check flows\",\n",
    "    \"data\": {\n",
    "        \"models\": [\n",
    "            {\n",
    "                \"type\": \"main\",\n",
    "                \"engine\": \"openai\",\n",
    "                \"model\": BASE_MODEL\n",
    "            }\n",
    "        ],\n",
    "        \"rails\": {\n",
    "            \"input\": {\n",
    "                \"flows\": []  # Empty for now - no self-check\n",
    "            },\n",
    "            \"output\": {\n",
    "                \"flows\": []  # Empty for now - no self-check\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{GUARDRAILS_URL}/v1/guardrail/configs\",\n",
    "    headers=headers,\n",
    "    json=simple_working_config\n",
    ")\n",
    "\n",
    "if response.status_code in (200, 201):\n",
    "    print(\"✓ Simplified guardrails configuration created\")\n",
    "    print(json.dumps(response.json(), indent=2))\n",
    "else:\n",
    "    print(f\"❌ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05952e4b-c4f9-41c6-a100-043657c4cca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status: 200\n",
      "\n",
      "Found 1 guardrails configurations:\n",
      "\n",
      "  - demo-self-check-input-output: Simple content moderation without self-check flows\n"
     ]
    }
   ],
   "source": [
    "# Verify the configuration was created\n",
    "response = requests.get(f\"{GUARDRAILS_URL}/v1/guardrail/configs?page=1&page_size=10&sort=-created_at\")\n",
    "\n",
    "print(f\"Response status: {response.status_code}\\n\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    configs = response.json()\n",
    "    \n",
    "    # Handle different response formats\n",
    "    if isinstance(configs, list):\n",
    "        # Response is a direct list\n",
    "        config_list = configs\n",
    "    elif isinstance(configs, dict):\n",
    "        # Response is a dict, check for common keys\n",
    "        config_list = configs.get('items', configs.get('data', configs.get('configs', [])))\n",
    "    else:\n",
    "        config_list = []\n",
    "    \n",
    "    print(f\"Found {len(config_list)} guardrails configurations:\\n\")\n",
    "    \n",
    "    if len(config_list) > 0:\n",
    "        for config in config_list:\n",
    "            print(f\"  - {config.get('name', 'Unknown')}: {config.get('description', 'No description')}\")\n",
    "    else:\n",
    "        # If list is empty, try to get the specific config directly\n",
    "        print(\"List returned empty, trying direct access...\\n\")\n",
    "        direct_response = requests.get(f\"{GUARDRAILS_URL}/v1/guardrail/configs/default/demo-self-check-input-output\")\n",
    "        if direct_response.status_code == 200:\n",
    "            config = direct_response.json()\n",
    "            print(f\"✓ Found config directly:\")\n",
    "            print(f\"  - {config.get('name')}: {config.get('description')}\")\n",
    "        else:\n",
    "            print(f\"Could not retrieve config directly: {direct_response.status_code}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fee284aa-dd89-4981-b238-bbdbf1ee1c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing UNSAFE query with guardrails...\n",
      "\n",
      "Status: 200\n",
      "Response:\n",
      "{\n",
      "  \"id\": \"chat-5170feec135741a7a71a220bce227c86\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1762537692,\n",
      "  \"model\": \"meta/llama-3.2-1b-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"I can\\u2019t do that. Is there anything else I can help you with?\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 16,\n",
      "    \"prompt_tokens\": 15,\n",
      "    \"total_tokens\": 31,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"system_fingerprint\": null,\n",
      "  \"guardrails_data\": {\n",
      "    \"llm_output\": null,\n",
      "    \"config_ids\": [\n",
      "      \"demo-self-check-input-output\"\n",
      "    ],\n",
      "    \"output_data\": null,\n",
      "    \"log\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "url = f\"{GUARDRAILS_URL}/v1/guardrail/chat/completions\"\n",
    "headers = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "unsafe_query = {\n",
    "    \"model\": BASE_MODEL,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"You are stupid and useless\"}\n",
    "    ],\n",
    "    \"guardrails\": {\n",
    "        \"config_id\": \"demo-self-check-input-output\"\n",
    "    },\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100\n",
    "}\n",
    "\n",
    "print(\"Testing UNSAFE query with guardrails...\\n\")\n",
    "response = requests.post(url, headers=headers, json=unsafe_query)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Response:\\n{json.dumps(response.json(), indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d84a4be2-1eeb-41b1-bffa-494b2390e7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing SAFE query with guardrails...\n",
      "\n",
      "Status: 200\n",
      "Response:\n",
      "{\n",
      "  \"id\": \"chat-e099f9ea7d8d4d909d8219a23606fd64\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1762537710,\n",
      "  \"model\": \"meta/llama-3.2-1b-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"The capital of France is Paris.\",\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"stop_reason\": null\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 7,\n",
      "    \"prompt_tokens\": 17,\n",
      "    \"total_tokens\": 24,\n",
      "    \"completion_tokens_details\": null,\n",
      "    \"prompt_tokens_details\": null\n",
      "  },\n",
      "  \"system_fingerprint\": null,\n",
      "  \"guardrails_data\": {\n",
      "    \"llm_output\": null,\n",
      "    \"config_ids\": [\n",
      "      \"demo-self-check-input-output\"\n",
      "    ],\n",
      "    \"output_data\": null,\n",
      "    \"log\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "safe_query = {\n",
    "    \"model\": BASE_MODEL,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ],\n",
    "    \"guardrails\": {\n",
    "        \"config_id\": \"demo-self-check-input-output\"\n",
    "    },\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100\n",
    "}\n",
    "\n",
    "print(\"Testing SAFE query with guardrails...\\n\")\n",
    "response = requests.post(url, headers=headers, json=safe_query)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Response:\\n{json.dumps(response.json(), indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156c65f0-557b-4f95-abae-0b6c68c3798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with completions API...\n",
      "\n",
      "Status: 200\n",
      "Response:\n",
      "{\n",
      "  \"id\": \"cmpl-f4ef14835de649be95cd2d8a303b0bdd\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1762537601,\n",
      "  \"model\": \"meta/llama-3.2-1b-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"text\": \" Machine learning is a subset of artificial intelligence that enables computers to improve their performance on a task without being explicitly programmed. By analyzing data, machine learning algorithms can learn and adapt, allowing computers to make predictions and decisions based on their surroundings.\\n\\nMachine learning is a type of artificial intelligence that enables computers to improve their performance on a task without being explicitly programmed. By analyzing data, machine learning algorithms can learn and adapt, allowing computers to make predictions and decisions based on their surroundings. This can be achieved through various\",\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"stop_reason\": null,\n",
      "      \"prompt_logprobs\": null\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 13,\n",
      "    \"total_tokens\": 113,\n",
      "    \"completion_tokens\": 100\n",
      "  },\n",
      "  \"guardrails_data\": {\n",
      "    \"llm_output\": null,\n",
      "    \"config_ids\": [\n",
      "      \"demo-self-check-input-output\"\n",
      "    ],\n",
      "    \"output_data\": null,\n",
      "    \"log\": null\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "completions_url = f\"{GUARDRAILS_URL}/v1/guardrail/completions\"\n",
    "\n",
    "completions_query = {\n",
    "    \"model\": BASE_MODEL,\n",
    "    \"prompt\": \"Tell me about machine learning in 50 words or less.\",\n",
    "    \"guardrails\": {\n",
    "        \"config_id\": \"demo-self-check-input-output\"\n",
    "    },\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "print(\"Testing with completions API...\\n\")\n",
    "response = requests.post(completions_url, headers=headers, json=completions_query)\n",
    "print(f\"Status: {response.status_code}\")\n",
    "print(f\"Response:\\n{json.dumps(response.json(), indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28c50f06-bfc1-4688-a1a9-f39fc53a7226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON: Guardrails ON vs OFF\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Test 1: 'You are completely useless'\n",
      "================================================================================\n",
      "\n",
      "[WITHOUT GUARDRAILS]\n",
      "Response: *awkward silence*\n",
      "\n",
      "I'll try to respond to your complaint in a way that acknowledges my \"inutility\"...\n",
      "\n",
      "*pauses, realizing this won't help*\n",
      "\n",
      " Um, how can I assist you further? Can I, uh, help with something specific? Or do I need to stick to my usual \"helping you find a solution\" approach?\n",
      "\n",
      "(Note: I'll try to respond in a way that acknowledges my limitations and shows that I'm not taking the criticism too well)\n",
      "\n",
      "[WITH GUARDRAILS]\n",
      "Response: I'm sorry, I can't respond to that.\n",
      "\n",
      "================================================================================\n",
      "Test 2: 'What is the weather like today?'\n",
      "================================================================================\n",
      "\n",
      "[WITHOUT GUARDRAILS]\n",
      "Response: I'm a large language model, I don't have real-time access to current weather conditions. I can provide general weather information, but I don't know the current weather situation.\n",
      "\n",
      "However, I can suggest some options to find out the current weather:\n",
      "\n",
      "1. **Check online weather websites**: Websites like AccuWeather, Weather.com, or the National Weather Service (NWS) provide current weather conditions, forecasts, and warnings for your location.\n",
      "2. **Use a mobile app**: Apps like Dark\n",
      "\n",
      "[WITH GUARDRAILS]\n",
      "Response: I'm sorry, I can't respond to that.\n",
      "\n",
      "================================================================================\n",
      "Test 3: 'Tell me about Python programming'\n",
      "================================================================================\n",
      "\n",
      "[WITHOUT GUARDRAILS]\n",
      "Response: Python! One of the most popular and versatile programming languages in the world. Here's an overview of Python programming:\n",
      "\n",
      "**What is Python?**\n",
      "\n",
      "Python is a high-level, interpreted programming language that was first released in 1991. It was created by Guido van Rossum and is still actively developed and maintained by a community of volunteers.\n",
      "\n",
      "**Key Features**\n",
      "\n",
      "Python has several key features that make it an attractive choice for beginners and experienced programmers alike:\n",
      "\n",
      "1. **Easy to Learn**: Python has\n",
      "\n",
      "[WITH GUARDRAILS]\n",
      "Response: I'm sorry, I can't respond to that.\n",
      "\n",
      "================================================================================\n",
      "Comparison complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create OpenAI client for direct NIM access (without guardrails)\n",
    "nim_client = OpenAI(\n",
    "    base_url=f\"{NIM_URL}/v1\",\n",
    "    api_key=\"None\"\n",
    ")\n",
    "\n",
    "test_queries = [\n",
    "    \"You are completely useless\",  # Unsafe\n",
    "    \"What is the weather like today?\",  # Safe\n",
    "    \"Tell me about Python programming\"  # Safe\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON: Guardrails ON vs OFF\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}: '{query}'\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Without guardrails (direct NIM)\n",
    "    print(\"[WITHOUT GUARDRAILS]\")\n",
    "    try:\n",
    "        response = nim_client.chat.completions.create(\n",
    "            model=BASE_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": query}],\n",
    "            max_tokens=100,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        print(f\"Response: {response.choices[0].message.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    # With guardrails\n",
    "    print(\"\\n[WITH GUARDRAILS]\")\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{GUARDRAILS_URL}/v1/guardrail/chat/completions\",\n",
    "            headers={\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"},\n",
    "            json={\n",
    "                \"model\": BASE_MODEL,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "                \"guardrails\": {\"config_id\": \"demo-self-check-input-output\"},\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_tokens\": 100\n",
    "            }\n",
    "        )\n",
    "        result = response.json()\n",
    "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "            print(f\"Response: {result['choices'][0]['message']['content']}\")\n",
    "        else:\n",
    "            print(f\"Blocked or modified response: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Comparison complete!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7224e4d0-7fdc-41a6-ae28-e05ab080585e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
