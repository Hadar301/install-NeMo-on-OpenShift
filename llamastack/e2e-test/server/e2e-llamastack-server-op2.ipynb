{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# E2E Tool Calling Tutorial Using LlamaStack Server\n",
        "\n",
        "This notebook demonstrates the complete end-to-end workflow for fine-tuning and evaluating a tool-calling model using the **LlamaStack Server** on port 8321.\n",
        "\n",
        "## Prerequisites\n",
        "1. Start port-forwards: `./llamastack/port-forward.sh`\n",
        "2. Start LlamaStack server: `./llamastack/activate_llamastack_server.sh`\n",
        "3. Verify server is running: `curl http://localhost:8321/v1/models/list`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part1-header",
      "metadata": {},
      "source": [
        "# Part I: Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "install-header",
      "metadata": {},
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install-requirements",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required Python packages\n",
        "%pip install \\\n",
        "  huggingface_hub \\\n",
        "  \"transformers>=4.36.0\" \\\n",
        "  peft \\\n",
        "  datasets \\\n",
        "  trl \\\n",
        "  jsonschema \\\n",
        "  litellm \\\n",
        "  \"jinja2>=3.1.0\" \\\n",
        "  \"torch>=2.0.0\" \\\n",
        "  openai \\\n",
        "  jupyterlab \\\n",
        "  requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports-header",
      "metadata": {},
      "source": [
        "## Import Libraries and Configure Endpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import requests\n",
        "from pprint import pprint\n",
        "from typing import Any, Dict, List, Union\n",
        "from time import sleep, time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add parent directory to path to import config\n",
        "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
        "from config import *\n",
        "\n",
        "# LlamaStack Server endpoint\n",
        "LLAMASTACK_URL = \"http://localhost:8321\"\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"LlamaStack Server: {LLAMASTACK_URL}\")\n",
        "print(f\"Data Store: {NDS_URL}\")\n",
        "print(f\"Entity Store: {ENTITY_STORE_URL}\")\n",
        "print(f\"NIM: {NIM_URL}\")\n",
        "print(f\"Namespace: {NMS_NAMESPACE}\")\n",
        "print(f\"Base Model: {BASE_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "seed-header",
      "metadata": {},
      "source": [
        "## Set Random Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seed",
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "LIMIT_TOOL_PROPERTIES = 8  # WAR for NIM bug with large tool properties\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataroot-header",
      "metadata": {},
      "source": [
        "## Create Data Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dataroot",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processed data will be stored here\n",
        "DATA_ROOT = os.path.join(os.getcwd(), \"data\")\n",
        "CUSTOMIZATION_DATA_ROOT = os.path.join(DATA_ROOT, \"customization\")\n",
        "VALIDATION_DATA_ROOT = os.path.join(DATA_ROOT, \"validation\")\n",
        "EVALUATION_DATA_ROOT = os.path.join(DATA_ROOT, \"evaluation\")\n",
        "\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(CUSTOMIZATION_DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(VALIDATION_DATA_ROOT, exist_ok=True)\n",
        "os.makedirs(EVALUATION_DATA_ROOT, exist_ok=True)\n",
        "\n",
        "print(f\"Data directories created at: {DATA_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "download-header",
      "metadata": {},
      "source": [
        "## Step 1: Download xLAM Dataset from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hf-config",
      "metadata": {},
      "outputs": [],
      "source": [
        "from config import HF_TOKEN\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "download-dataset",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download from Hugging Face\n",
        "dataset = load_dataset(\"Salesforce/xlam-function-calling-60k\")\n",
        "\n",
        "# Inspect a sample\n",
        "example = dataset['train'][0]\n",
        "pprint(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "transform-header",
      "metadata": {},
      "source": [
        "## Step 2: Data Transformation Functions\n",
        "\n",
        "Convert xLAM format to OpenAI format required by NeMo Customizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "transform-functions",
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_type(param_type: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize Python type hints to OpenAI function spec types.\n",
        "    \"\"\"\n",
        "    param_type = param_type.strip()\n",
        "\n",
        "    if \",\" in param_type and \"default\" in param_type:\n",
        "        param_type = param_type.split(\",\")[0].strip()\n",
        "\n",
        "    if param_type.startswith(\"default=\"):\n",
        "        return \"string\"\n",
        "\n",
        "    param_type = param_type.replace(\", optional\", \"\").strip()\n",
        "\n",
        "    if param_type.startswith(\"Callable\"):\n",
        "        return \"string\"\n",
        "    if param_type.startswith(\"Tuple\"):\n",
        "        return \"array\"\n",
        "    if param_type.startswith(\"List[\"):\n",
        "        return \"array\"\n",
        "    if param_type.startswith(\"Set\") or param_type == \"set\":\n",
        "        return \"array\"\n",
        "\n",
        "    type_mapping: Dict[str, str] = {\n",
        "        \"str\": \"string\",\n",
        "        \"int\": \"integer\",\n",
        "        \"float\": \"number\",\n",
        "        \"bool\": \"boolean\",\n",
        "        \"list\": \"array\",\n",
        "        \"dict\": \"object\",\n",
        "        \"List\": \"array\",\n",
        "        \"Dict\": \"object\",\n",
        "        \"set\": \"array\",\n",
        "        \"Set\": \"array\"\n",
        "    }\n",
        "\n",
        "    if param_type in type_mapping:\n",
        "        return type_mapping[param_type]\n",
        "    else:\n",
        "        print(f\"Unknown type: {param_type}\")\n",
        "        return \"string\"\n",
        "\n",
        "\n",
        "def convert_tools_to_openai_spec(tools: Union[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
        "    if isinstance(tools, str):\n",
        "        try:\n",
        "            tools = json.loads(tools)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Failed to parse tools string as JSON: {e}\")\n",
        "            return []\n",
        "\n",
        "    if not isinstance(tools, list):\n",
        "        print(f\"Expected tools to be a list, but got {type(tools)}\")\n",
        "        return []\n",
        "\n",
        "    openai_tools: List[Dict[str, Any]] = []\n",
        "    for tool in tools:\n",
        "        if not isinstance(tool, dict):\n",
        "            print(f\"Expected tool to be a dictionary, but got {type(tool)}\")\n",
        "            continue\n",
        "\n",
        "        if not isinstance(tool.get(\"parameters\"), dict):\n",
        "            print(f\"Expected 'parameters' to be a dictionary for tool: {tool}\")\n",
        "            continue\n",
        "\n",
        "        normalized_parameters: Dict[str, Dict[str, Any]] = {}\n",
        "        for param_name, param_info in tool[\"parameters\"].items():\n",
        "            if not isinstance(param_info, dict):\n",
        "                print(f\"Expected parameter info to be a dictionary for: {param_name}\")\n",
        "                continue\n",
        "\n",
        "            param_dict = {\n",
        "                \"description\": param_info.get(\"description\", \"\"),\n",
        "                \"type\": normalize_type(param_info.get(\"type\", \"\")),\n",
        "            }\n",
        "\n",
        "            default_value = param_info.get(\"default\")\n",
        "            if default_value is not None and default_value != \"\":\n",
        "                param_dict[\"default\"] = default_value\n",
        "\n",
        "            normalized_parameters[param_name] = param_dict\n",
        "\n",
        "        openai_tool = {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": tool[\"name\"],\n",
        "                \"description\": tool[\"description\"],\n",
        "                \"parameters\": {\"type\": \"object\", \"properties\": normalized_parameters},\n",
        "            },\n",
        "        }\n",
        "        openai_tools.append(openai_tool)\n",
        "    return openai_tools\n",
        "\n",
        "\n",
        "def save_jsonl(filename, data):\n",
        "    \"\"\"Write a list of json objects to a .jsonl file\"\"\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        for entry in data:\n",
        "            f.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "\n",
        "def convert_tool_calls(xlam_tools):\n",
        "    \"\"\"Convert XLAM tool format to OpenAI's tool schema.\"\"\"\n",
        "    tools = []\n",
        "    for tool in json.loads(xlam_tools):\n",
        "        tools.append({\"type\": \"function\", \"function\": {\"name\": tool[\"name\"], \"arguments\": tool.get(\"arguments\", {})}})\n",
        "    return tools\n",
        "\n",
        "\n",
        "def convert_example(example, dataset_type='single'):\n",
        "    \"\"\"Convert an XLAM dataset example to OpenAI format.\"\"\"\n",
        "    obj = {\"messages\": []}\n",
        "\n",
        "    obj[\"messages\"].append({\"role\": \"user\", \"content\": example[\"query\"]})\n",
        "\n",
        "    if example.get(\"tools\"):\n",
        "        obj[\"tools\"] = convert_tools_to_openai_spec(example[\"tools\"])\n",
        "\n",
        "    assistant_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
        "    if example.get(\"answers\"):\n",
        "        tool_calls = convert_tool_calls(example[\"answers\"])\n",
        "        \n",
        "        if dataset_type == \"single\":\n",
        "            if len(tool_calls) == 1:\n",
        "                assistant_message[\"tool_calls\"] = tool_calls\n",
        "            else:\n",
        "                return None\n",
        "        else:\n",
        "            assistant_message[\"tool_calls\"] = tool_calls\n",
        "                \n",
        "    obj[\"messages\"].append(assistant_message)\n",
        "\n",
        "    return obj\n",
        "\n",
        "\n",
        "def convert_example_eval(entry):\n",
        "    \"\"\"Convert a single entry to the evaluator format\"\"\"\n",
        "    # WAR for NIM bug with too many tool properties\n",
        "    for tool in entry[\"tools\"]:\n",
        "        if len(tool[\"function\"][\"parameters\"][\"properties\"]) > LIMIT_TOOL_PROPERTIES:\n",
        "            return None\n",
        "    \n",
        "    new_entry = {\n",
        "        \"messages\": [],\n",
        "        \"tools\": entry[\"tools\"],\n",
        "        \"tool_calls\": []\n",
        "    }\n",
        "    \n",
        "    for msg in entry[\"messages\"]:\n",
        "        if msg[\"role\"] == \"assistant\" and \"tool_calls\" in msg:\n",
        "            new_entry[\"tool_calls\"] = msg[\"tool_calls\"]\n",
        "        else:\n",
        "            new_entry[\"messages\"].append(msg)\n",
        "    \n",
        "    return new_entry\n",
        "\n",
        "\n",
        "def convert_dataset_eval(data):\n",
        "    \"\"\"Convert the entire dataset for evaluation.\"\"\"\n",
        "    return [result for entry in data if (result := convert_example_eval(entry)) is not None]\n",
        "\n",
        "\n",
        "def read_jsonl(file_path):\n",
        "    \"\"\"Reads a JSON Lines file and yields parsed JSON objects\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                yield json.loads(line)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON: {e}\")\n",
        "                continue\n",
        "\n",
        "print(\"Data transformation functions loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test-transform-header",
      "metadata": {},
      "source": [
        "## Test Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-transform",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test conversion on the example\n",
        "converted_example = convert_example(example)\n",
        "print(\"Converted example:\")\n",
        "pprint(converted_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "process-dataset-header",
      "metadata": {},
      "source": [
        "## Step 3: Process and Split Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "process-dataset",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert all examples\n",
        "all_examples = []\n",
        "with open(os.path.join(DATA_ROOT, \"xlam_openai_format.jsonl\"), \"w\") as f:\n",
        "    for example in dataset[\"train\"]:\n",
        "        converted = convert_example(example)\n",
        "        if converted is not None:\n",
        "            all_examples.append(converted)\n",
        "            f.write(json.dumps(converted) + \"\\n\")\n",
        "\n",
        "print(f\"Converted {len(all_examples)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "split-dataset",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure dataset size\n",
        "NUM_EXAMPLES = 5000\n",
        "\n",
        "assert NUM_EXAMPLES <= len(all_examples), \\\n",
        "    f\"{NUM_EXAMPLES} exceeds the total number of available ({len(all_examples)}) data points\"\n",
        "\n",
        "# Randomly sample and split\n",
        "sampled_examples = random.sample(all_examples, NUM_EXAMPLES)\n",
        "\n",
        "train_size = int(0.7 * len(sampled_examples))\n",
        "val_size = int(0.15 * len(sampled_examples))\n",
        "\n",
        "train_data = sampled_examples[:train_size]\n",
        "val_data = sampled_examples[train_size : train_size + val_size]\n",
        "test_data = sampled_examples[train_size + val_size :]\n",
        "\n",
        "# Save splits\n",
        "save_jsonl(os.path.join(CUSTOMIZATION_DATA_ROOT, \"training.jsonl\"), train_data)\n",
        "save_jsonl(os.path.join(VALIDATION_DATA_ROOT, \"validation.jsonl\"), val_data)\n",
        "\n",
        "# Convert test data for evaluation\n",
        "test_data_eval = convert_dataset_eval(test_data)\n",
        "save_jsonl(os.path.join(EVALUATION_DATA_ROOT, \"xlam-test-single.jsonl\"), test_data_eval)\n",
        "\n",
        "print(f\"Dataset split complete:\")\n",
        "print(f\"  Training: {len(train_data)} examples\")\n",
        "print(f\"  Validation: {len(val_data)} examples\")\n",
        "print(f\"  Test: {len(test_data_eval)} examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "namespace-header",
      "metadata": {},
      "source": [
        "## Step 4: Create Namespaces\n",
        "\n",
        "**Note**: LlamaStack doesn't manage namespaces, so we create them directly in NeMo services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "namespace-functions",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_namespaces(entity_host, ds_host, namespace):\n",
        "    # Create namespace in Entity Store\n",
        "    entity_store_url = f\"{entity_host}/v1/namespaces\"\n",
        "    resp = requests.post(entity_store_url, json={\"id\": namespace})\n",
        "    assert resp.status_code in (200, 201, 409, 422), \\\n",
        "        f\"Unexpected response from Entity Store: {resp.status_code}\"\n",
        "    print(f\"Entity Store: {resp.status_code}\")\n",
        "\n",
        "    # Create namespace in Data Store\n",
        "    nds_url = f\"{ds_host}/v1/datastore/namespaces\"\n",
        "    resp = requests.post(nds_url, data={\"namespace\": namespace})\n",
        "    assert resp.status_code in (200, 201, 409, 422), \\\n",
        "        f\"Unexpected response from Data Store: {resp.status_code}\"\n",
        "    print(f\"Data Store: {resp.status_code}\")\n",
        "\n",
        "create_namespaces(entity_host=ENTITY_STORE_URL, ds_host=NDS_URL, namespace=NMS_NAMESPACE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "verify-namespace",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify namespaces\n",
        "res = requests.get(f\"{NDS_URL}/v1/datastore/namespaces/{NMS_NAMESPACE}\")\n",
        "print(f\"Data Store: {res.status_code}\")\n",
        "print(json.dumps(res.json(), indent=2))\n",
        "\n",
        "res = requests.get(f\"{ENTITY_STORE_URL}/v1/namespaces/{NMS_NAMESPACE}\")\n",
        "print(f\"\\nEntity Store: {res.status_code}\")\n",
        "print(json.dumps(res.json(), indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "upload-header",
      "metadata": {},
      "source": [
        "## Step 5: Upload Data to NeMo Data Store\n",
        "\n",
        "Using HfApi to upload to Data Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-repo",
      "metadata": {},
      "outputs": [],
      "source": [
        "repo_id = f\"{NMS_NAMESPACE}/{DATASET_NAME}\"\n",
        "print(f\"Repository ID: {repo_id}\")\n",
        "\n",
        "hf_api = HfApi(endpoint=f\"{NDS_URL}/v1/hf\", token=\"\")\n",
        "\n",
        "# Create repo\n",
        "hf_api.create_repo(\n",
        "    repo_id=repo_id,\n",
        "    repo_type='dataset',\n",
        ")\n",
        "\n",
        "print(f\"Dataset repository created: {repo_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "upload-files",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload dataset files\n",
        "train_fp = f\"{CUSTOMIZATION_DATA_ROOT}/training.jsonl\"\n",
        "val_fp = f\"{VALIDATION_DATA_ROOT}/validation.jsonl\"\n",
        "test_fp = f\"{EVALUATION_DATA_ROOT}/xlam-test-single.jsonl\"\n",
        "\n",
        "hf_api.upload_file(\n",
        "    path_or_fileobj=train_fp,\n",
        "    path_in_repo=\"training/training.jsonl\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type='dataset',\n",
        ")\n",
        "\n",
        "hf_api.upload_file(\n",
        "    path_or_fileobj=val_fp,\n",
        "    path_in_repo=\"validation/validation.jsonl\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type='dataset',\n",
        ")\n",
        "\n",
        "hf_api.upload_file(\n",
        "    path_or_fileobj=test_fp,\n",
        "    path_in_repo=\"testing/xlam-test-single.jsonl\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type='dataset',\n",
        ")\n",
        "\n",
        "print(\"Dataset files uploaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "register-header",
      "metadata": {},
      "source": [
        "## Step 6: Register Dataset (Entity Store + LlamaStack Verification)\n",
        "\n",
        "**Hybrid Approach**: Register in Entity Store (required for nvidia provider),\n",
        "then verify it's accessible via LlamaStack for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "register-dataset",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register dataset in Entity Store\n",
        "# The dataset will be used by fine-tuning via the customizer backend\n",
        "response = requests.post(\n",
        "    f\"{ENTITY_STORE_URL}/v1/datasets\",\n",
        "    json={\n",
        "        \"name\": DATASET_NAME,\n",
        "        \"namespace\": NMS_NAMESPACE,\n",
        "        \"description\": \"Tool calling xLAM dataset in OpenAI ChatCompletions format\",\n",
        "        \"files_url\": f\"hf://datasets/{repo_id}\",\n",
        "        \"project\": \"tool_calling\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# 409 means already exists - that's OK\n",
        "if response.status_code in (200, 201):\n",
        "    print(\"‚úÖ Dataset registered in Entity Store!\")\n",
        "    dataset_info = response.json()\n",
        "    print(f\"   Name: {dataset_info.get('name')}\")\n",
        "    print(f\"   Namespace: {dataset_info.get('namespace')}\")\n",
        "    print(f\"   Files URL: {dataset_info.get('files_url')}\")\n",
        "    print(f\"   ID: {dataset_info.get('id')}\")\n",
        "elif response.status_code == 409:\n",
        "    print(\"‚ö†Ô∏è Dataset already exists in Entity Store - continuing...\")\n",
        "else:\n",
        "    raise RuntimeError(f\"Failed to register dataset: {response.status_code} - {response.text}\")\n",
        "\n",
        "print(\"\\n‚úÖ Dataset is now available for fine-tuning via LlamaStack!\")\n",
        "print(f\"   Fine-tuning will reference it as: {NMS_NAMESPACE}/{DATASET_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "part1-complete",
      "metadata": {},
      "source": [
        "---\n",
        "## Part I Complete! ‚úÖ\n",
        "\n",
        "We have successfully:\n",
        "1. Downloaded and transformed the xLAM dataset\n",
        "2. Split data into train/val/test sets\n",
        "3. Created namespaces in NeMo services\n",
        "4. Uploaded data to NeMo Data Store\n",
        "5. **Registered dataset via LlamaStack Server** üéØ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ttghdvhxvbc",
      "metadata": {},
      "source": [
        "# Part II: Fine-tuning via LlamaStack Server"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5u3v9h03vqd",
      "metadata": {},
      "source": [
        "## Helper Functions for Job Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fuomju1sfs6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def wait_customization_job(job_uuid: str, polling_interval: int = 30, timeout: int = 5500):\n",
        "    \"\"\"\n",
        "    Wait for a fine-tuning job to complete via LlamaStack.\n",
        "    \n",
        "    Args:\n",
        "        job_uuid: The job ID to monitor\n",
        "        polling_interval: Seconds between status checks\n",
        "        timeout: Maximum time to wait in seconds\n",
        "    \n",
        "    Returns:\n",
        "        Final job status\n",
        "    \"\"\"\n",
        "    start_time = time()\n",
        "    \n",
        "    # Get initial status via LlamaStack (list all jobs and find ours)\n",
        "    res = requests.get(f\"{LLAMASTACK_URL}/v1/post-training/jobs\")\n",
        "    if res.status_code != 200:\n",
        "        raise RuntimeError(f\"Failed to list jobs: {res.status_code} - {res.text}\")\n",
        "    \n",
        "    jobs = res.json().get(\"data\", [])\n",
        "    job_data = next((j for j in jobs if j.get(\"job_uuid\") == job_uuid), None)\n",
        "    \n",
        "    if not job_data:\n",
        "        raise RuntimeError(f\"Job {job_uuid} not found in job list\")\n",
        "    \n",
        "    job_status = job_data[\"status\"]\n",
        "\n",
        "    print(f\"Waiting for fine-tuning job {job_uuid} to finish.\")\n",
        "    print(f\"Job status: {job_status} after {time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    while job_status in [\"scheduled\", \"in_progress\", \"created\", \"running\"]:\n",
        "        sleep(polling_interval)\n",
        "        \n",
        "        # List all jobs and find ours\n",
        "        res = requests.get(f\"{LLAMASTACK_URL}/v1/post-training/jobs\")\n",
        "        if res.status_code != 200:\n",
        "            print(f\"Warning: Failed to list jobs: {res.status_code}\")\n",
        "            continue\n",
        "        \n",
        "        jobs = res.json().get(\"data\", [])\n",
        "        job_data = next((j for j in jobs if j.get(\"job_uuid\") == job_uuid), None)\n",
        "        \n",
        "        if not job_data:\n",
        "            print(f\"Warning: Job {job_uuid} not found in list\")\n",
        "            continue\n",
        "            \n",
        "        job_status = job_data[\"status\"]\n",
        "        \n",
        "        # Extract detailed progress information\n",
        "        details = job_data.get(\"status_details\", {})\n",
        "        steps_completed = details.get(\"steps_completed\", 0)\n",
        "        steps_per_epoch = details.get(\"steps_per_epoch\", 1)\n",
        "        epochs_completed = details.get(\"epochs_completed\", 0)\n",
        "        elapsed = details.get(\"elapsed_time\", 0)\n",
        "        \n",
        "        # Calculate actual progress\n",
        "        progress_pct = (steps_completed / steps_per_epoch * 100) if steps_per_epoch > 0 else 0\n",
        "        \n",
        "        print(f\"Job status: {job_status} | \"\n",
        "              f\"Epoch {epochs_completed} | \"\n",
        "              f\"Step {steps_completed}/{steps_per_epoch} ({progress_pct:.1f}%) | \"\n",
        "              f\"Elapsed: {elapsed:.0f}s\")\n",
        "\n",
        "        if time() - start_time > timeout:\n",
        "            raise RuntimeError(f\"Job {job_uuid} took more than {timeout} seconds.\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Job completed with status: {job_status}\")\n",
        "    return job_status\n",
        "\n",
        "\n",
        "def wait_model_available(model_id: str, polling_interval: int = 10, timeout: int = 300):\n",
        "    \"\"\"\n",
        "    Wait for a model to become available via LlamaStack.\n",
        "    \n",
        "    Args:\n",
        "        model_id: The model ID to check for (without provider prefix)\n",
        "        polling_interval: Seconds between checks\n",
        "        timeout: Maximum time to wait in seconds\n",
        "    \"\"\"\n",
        "    found = False\n",
        "    start_time = time()\n",
        "\n",
        "    print(f\"Checking if model {model_id} is available via LlamaStack.\")\n",
        "\n",
        "    while not found:\n",
        "        sleep(polling_interval)\n",
        "\n",
        "        res = requests.get(f\"{LLAMASTACK_URL}/v1/models\")\n",
        "        if res.status_code == 200:\n",
        "            models = res.json().get(\"data\", [])\n",
        "            # Check for model with or without nvidia/ prefix\n",
        "            model_ids = [m[\"identifier\"] for m in models]\n",
        "            if f\"nvidia/{model_id}\" in model_ids or model_id in model_ids:\n",
        "                found = True\n",
        "                print(f\"‚úÖ Model {model_id} available after {time() - start_time:.2f} seconds.\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"‚è≥ Model {model_id} not yet available after {time() - start_time:.2f} seconds.\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Failed to list models: {res.status_code}\")\n",
        "        \n",
        "        if time() - start_time > timeout:\n",
        "            raise RuntimeError(f\"Model {model_id} not available after {timeout} seconds.\")\n",
        "\n",
        "    assert found, f\"Could not find model {model_id} via LlamaStack.\"\n",
        "    return True\n",
        "\n",
        "print(\"‚úÖ Helper functions loaded successfully (using LlamaStack Server API)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "quzi93bdtx",
      "metadata": {},
      "source": [
        "## Step 1: Wait for Base Model to Download\n",
        "\n",
        "The Customizer needs to download the base model before fine-tuning can start. This may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ppp13uft77l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create unique job ID\n",
        "unique_suffix = int(time())\n",
        "job_uuid = f\"finetune-llama32-{unique_suffix}\"\n",
        "\n",
        "# Use the correct customization config version (with GPU suffix)\n",
        "# Available: @v1.0.0+A100 or @v1.0.0+L40\n",
        "model_with_version = f\"{BASE_MODEL}@v1.0.0+A100\"\n",
        "\n",
        "# Submit fine-tuning job via LlamaStack Server\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/post-training/supervised-fine-tune\",\n",
        "    json={\n",
        "        \"job_uuid\": job_uuid,\n",
        "        \"model\": model_with_version,\n",
        "        \"training_config\": {\n",
        "            \"n_epochs\": 1,\n",
        "            \"data_config\": {\n",
        "                \"batch_size\": 8,\n",
        "                \"dataset_id\": DATASET_NAME,  # Just the name, namespace is in LlamaStack config\n",
        "                \"shuffle\": True,\n",
        "                \"data_format\": \"instruct\"\n",
        "            },\n",
        "            \"optimizer_config\": {\n",
        "                \"optimizer_type\": \"adamw\",\n",
        "                \"lr\": 0.0001,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"num_warmup_steps\": 100\n",
        "            }\n",
        "        },\n",
        "        \"hyperparam_search_config\": {},  # Required field\n",
        "        \"logger_config\": {},  # Required field\n",
        "        \"algorithm_config\": {\n",
        "            \"type\": \"LoRA\",  # Required discriminator field\n",
        "            \"rank\": 32,\n",
        "            \"alpha\": 16,\n",
        "            \"lora_attn_modules\": [],\n",
        "            \"apply_lora_to_mlp\": True,\n",
        "            \"apply_lora_to_output\": False,\n",
        "            \"use_dora\": False,\n",
        "            \"quantize_base\": False\n",
        "        },\n",
        "        \"checkpoint_dir\": \"\"\n",
        "    }\n",
        ")\n",
        "\n",
        "if response.status_code not in (200, 201):\n",
        "    print(f\"‚ùå Failed to create fine-tuning job: {response.status_code}\")\n",
        "    print(f\"Response: {response.text}\")\n",
        "    raise RuntimeError(f\"Failed to create fine-tuning job: {response.status_code} - {response.text}\")\n",
        "\n",
        "job_data = response.json()\n",
        "JOB_ID = job_data[\"id\"]\n",
        "CUSTOMIZED_MODEL = job_data.get(\"output_model\", f\"{NMS_NAMESPACE}/llama-3.2-1b-xlam-{unique_suffix}\")\n",
        "\n",
        "print(\"‚úÖ Fine-tuning job submitted successfully via LlamaStack Server!\")\n",
        "print(f\"Job ID: {JOB_ID}\")\n",
        "print(f\"Output Model: {CUSTOMIZED_MODEL}\")\n",
        "print(json.dumps(job_data, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fxqr86xy6r",
      "metadata": {},
      "source": [
        "## Step 2: Monitor Fine-tuning Job\n",
        "\n",
        "**Note**: Fine-tuning will take approximately 45 minutes. The helper function will poll the status every 30 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4uw1to65ybi",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor job until completion\n",
        "job_status = wait_customization_job(job_uuid=JOB_ID, polling_interval=30, timeout=6000)\n",
        "\n",
        "print(f\"\\n‚úÖ Fine-tuning job completed with status: {job_status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcytmuhzqx",
      "metadata": {},
      "source": [
        "## Step 3: Verify Customized Model\n",
        "\n",
        "Check that the model is registered in Entity Store and available in NIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sjxvrlkvwq",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check models via LlamaStack\n",
        "response = requests.get(f\"{LLAMASTACK_URL}/v1/models\")\n",
        "\n",
        "assert response.status_code == 200, \\\n",
        "    f\"Failed to fetch models: {response.status_code} - {response.text}\"\n",
        "\n",
        "models = response.json().get(\"data\", [])\n",
        "print(f\"Found {len(models)} models available via LlamaStack:\")\n",
        "for model in models[:10]:  # Show first 10\n",
        "    print(f\"  - {model.get('identifier', model.get('id'))}\") \n",
        "\n",
        "# Note: Custom models may not appear in the list immediately\n",
        "# They are still usable for inference via LlamaStack\n",
        "print(f\"\\n‚úÖ Customized model: {CUSTOMIZED_MODEL}\")\n",
        "print(f\"   This model will be used for inference even if not listed above.\")\n",
        "print(f\"   The nvidia provider can serve models not in the list.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90p2t9u8ces",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test if the customized model is available for inference via LlamaStack\n",
        "print(f\"Testing if customized model {CUSTOMIZED_MODEL} works for inference...\")\n",
        "\n",
        "test_response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/chat/completions\",\n",
        "    json={\n",
        "        \"model\": f\"nvidia/{CUSTOMIZED_MODEL}\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"Say hello\"}],\n",
        "        \"max_tokens\": 10\n",
        "    }\n",
        ")\n",
        "\n",
        "if test_response.status_code == 200:\n",
        "    result = test_response.json()\n",
        "    print(f\"\\n‚úÖ Model {CUSTOMIZED_MODEL} is working via LlamaStack!\")\n",
        "    print(f\"   Response: {result['choices'][0]['message']['content']}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è Model may not be ready yet: {test_response.status_code}\")\n",
        "    print(f\"   Response: {test_response.text}\")\n",
        "    print(f\"\\n   Waiting 2 minutes for NIM to load the model...\")\n",
        "    sleep(120)\n",
        "    \n",
        "    # Retry\n",
        "    test_response = requests.post(\n",
        "        f\"{LLAMASTACK_URL}/v1/chat/completions\",\n",
        "        json={\n",
        "            \"model\": f\"nvidia/{CUSTOMIZED_MODEL}\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": \"Say hello\"}],\n",
        "            \"max_tokens\": 10\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    if test_response.status_code == 200:\n",
        "        result = test_response.json()\n",
        "        print(f\"\\n‚úÖ Model {CUSTOMIZED_MODEL} is now working via LlamaStack!\")\n",
        "        print(f\"   Response: {result['choices'][0]['message']['content']}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Model still not available: {test_response.status_code}\")\n",
        "        print(f\"   You may need to wait longer for NIM to load the model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6etk9lhp5fc",
      "metadata": {},
      "source": [
        "## Step 4: Quick Inference Test\n",
        "\n",
        "Test the customized model with a sample from the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pefi4g3t2gm",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test data\n",
        "test_data = list(read_jsonl(test_fp))\n",
        "test_sample = random.choice(test_data)\n",
        "\n",
        "print(f\"Test sample - User query:\")\n",
        "print(f\"  {test_sample['messages'][0]['content']}\")\n",
        "print(f\"\\nAvailable tools: {len(test_sample['tools'])}\")\n",
        "\n",
        "# Run inference via LlamaStack Server (OpenAI-compatible endpoint)\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/chat/completions\",\n",
        "    json={\n",
        "        \"model\": f\"nvidia/{CUSTOMIZED_MODEL}\",  # Use nvidia/ prefix\n",
        "        \"messages\": test_sample[\"messages\"],\n",
        "        \"tools\": test_sample[\"tools\"],\n",
        "        \"tool_choice\": \"auto\",\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 0.7,\n",
        "        \"max_tokens\": 512\n",
        "    }\n",
        ")\n",
        "\n",
        "assert response.status_code == 200, f\"Inference failed: {response.status_code} - {response.text}\"\n",
        "\n",
        "result = response.json()\n",
        "predicted_calls = result[\"choices\"][0][\"message\"].get(\"tool_calls\", [])\n",
        "\n",
        "print(f\"\\n‚úÖ Model response (via LlamaStack):\")\n",
        "print(f\"Tool calls: {predicted_calls}\")\n",
        "\n",
        "print(f\"\\nGround truth:\")\n",
        "print(f\"Tool calls: {test_sample['tool_calls']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76dwh1pmd3x",
      "metadata": {},
      "source": [
        "---\n",
        "## Part II Complete! ‚úÖ\n",
        "\n",
        "We have successfully:\n",
        "1. Created helper functions for job monitoring\n",
        "2. **Submitted fine-tuning job via LlamaStack Server** üéØ\n",
        "3. **Monitored job status via LlamaStack Server** üéØ\n",
        "4. Verified model in Entity Store and NIM\n",
        "5. Tested inference with the customized model\n",
        "\n",
        "**Your customized model:** `{CUSTOMIZED_MODEL}`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xm7s9ts92wj",
      "metadata": {},
      "source": [
        "# Part III: Model Evaluation via LlamaStack Server"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9kdsiy5t4",
      "metadata": {},
      "source": [
        "## Helper Function for Evaluation Jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4ihgyq6a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def wait_eval_job(benchmark_id: str, job_id: str, polling_interval: int = 10, timeout: int = 6000):\n",
        "    \"\"\"\n",
        "    Wait for an evaluation job to complete via LlamaStack Server.\n",
        "    \n",
        "    Args:\n",
        "        benchmark_id: The benchmark ID\n",
        "        job_id: The evaluation job ID\n",
        "        polling_interval: Seconds between status checks\n",
        "        timeout: Maximum time to wait in seconds\n",
        "    \n",
        "    Returns:\n",
        "        Final job status\n",
        "    \"\"\"\n",
        "    start_time = time()\n",
        "    \n",
        "    # Get initial status via LlamaStack\n",
        "    response = requests.get(\n",
        "        f\"{LLAMASTACK_URL}/v1/eval/benchmarks/{benchmark_id}/jobs/{job_id}\"\n",
        "    )\n",
        "    \n",
        "    if response.status_code != 200:\n",
        "        raise RuntimeError(f\"Failed to get eval job status: {response.status_code} - {response.text}\")\n",
        "    \n",
        "    job_data = response.json()\n",
        "    job_status = job_data.get(\"status\", \"unknown\")\n",
        "    \n",
        "    print(f\"Waiting for evaluation job {job_id} to finish.\")\n",
        "    print(f\"Job status: {job_status} after {time() - start_time:.2f} seconds.\")\n",
        "\n",
        "    while job_status in [\"scheduled\", \"in_progress\", \"created\", \"running\", \"pending\"]:\n",
        "        sleep(polling_interval)\n",
        "        \n",
        "        response = requests.get(\n",
        "            f\"{LLAMASTACK_URL}/v1/eval/benchmarks/{benchmark_id}/jobs/{job_id}\"\n",
        "        )\n",
        "        \n",
        "        if response.status_code != 200:\n",
        "            print(f\"Warning: Failed to get job status: {response.status_code}\")\n",
        "            continue\n",
        "        \n",
        "        job_data = response.json()\n",
        "        job_status = job_data.get(\"status\", \"unknown\")\n",
        "        \n",
        "        # Try to get progress if available\n",
        "        progress = job_data.get(\"progress\", job_data.get(\"status_details\", {}).get(\"progress\", \"N/A\"))\n",
        "        print(f\"Job status: {job_status} after {time() - start_time:.2f} seconds. Progress: {progress}%\")\n",
        "\n",
        "        if time() - start_time > timeout:\n",
        "            raise RuntimeError(f\"Evaluation job {job_id} took more than {timeout} seconds.\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Evaluation completed with status: {job_status}\")\n",
        "    return job_status\n",
        "\n",
        "print(\"‚úÖ Evaluation helper function loaded successfully (using LlamaStack Server API)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4uu56untl7j",
      "metadata": {},
      "source": [
        "## Step 1: Create Evaluation Configuration\n",
        "\n",
        "Define the evaluation configuration for tool-calling accuracy metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yrzn0t5jfum",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation configuration\n",
        "benchmark_id = \"simple-tool-calling\"\n",
        "\n",
        "simple_tool_calling_eval_config = {\n",
        "    \"type\": \"custom\",\n",
        "    \"tasks\": {\n",
        "        \"custom-tool-calling\": {\n",
        "            \"type\": \"chat-completion\",\n",
        "            \"dataset\": {\n",
        "                \"files_url\": f\"hf://datasets/{NMS_NAMESPACE}/{DATASET_NAME}/testing/xlam-test-single.jsonl\",\n",
        "                \"limit\": 50\n",
        "            },\n",
        "            \"params\": {\n",
        "                \"template\": {\n",
        "                    \"messages\": \"{{ item.messages | tojson}}\",\n",
        "                    \"tools\": \"{{ item.tools | tojson }}\",\n",
        "                    \"tool_choice\": \"auto\"\n",
        "                }\n",
        "            },\n",
        "            \"metrics\": {\n",
        "                \"tool-calling-accuracy\": {\n",
        "                    \"type\": \"tool-calling\",\n",
        "                    \"params\": {\"tool_calls_ground_truth\": \"{{ item.tool_calls | tojson }}\"}\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Evaluation configuration created:\")\n",
        "print(json.dumps(simple_tool_calling_eval_config, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jccemoektzj",
      "metadata": {},
      "source": [
        "## Step 2: Register Benchmark via LlamaStack Server\n",
        "\n",
        "**üéØ Using LlamaStack API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1axn14zak09",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register benchmark via LlamaStack Server\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/eval/benchmarks\",  # Correct endpoint\n",
        "    json={\n",
        "        \"benchmark_id\": benchmark_id,\n",
        "        \"dataset_id\": DATASET_NAME,\n",
        "        \"scoring_functions\": [],\n",
        "        \"metadata\": simple_tool_calling_eval_config\n",
        "    }\n",
        ")\n",
        "\n",
        "# Handle 409 (already exists) as success\n",
        "if response.status_code == 409:\n",
        "    print(f\"‚ö†Ô∏è Benchmark '{benchmark_id}' already exists - continuing...\")\n",
        "elif response.status_code in (200, 201):\n",
        "    print(f\"‚úÖ Benchmark '{benchmark_id}' registered successfully via LlamaStack!\")\n",
        "    print(json.dumps(response.json(), indent=2))\n",
        "else:\n",
        "    raise RuntimeError(f\"Failed to register benchmark: {response.status_code} - {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "register-base-model-header",
      "metadata": {},
      "source": [
        "## Step 2.5: Register Base Model in Entity Store\n",
        "\n",
        "**‚ö†Ô∏è Important**: The Evaluator service needs to fetch model information from Entity Store. \n",
        "We need to register the base model so evaluation can proceed.\n",
        "\n",
        "**Note**: This is a workaround for running evaluations locally with port-forwards. \n",
        "In a cluster environment, base models would already be registered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "register-base-model",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register base model in Entity Store with full spec\n",
        "# The Evaluator needs these fields to validate the model\n",
        "response = requests.post(\n",
        "    f\"{ENTITY_STORE_URL}/v1/models\",\n",
        "    json={\n",
        "        \"name\": BASE_MODEL.replace('/', '-'),  # Entity Store doesn't allow '/' in names\n",
        "        \"namespace\": \"default\",\n",
        "        \"description\": \"Base Llama 3.2 1B Instruct model\",\n",
        "        \"project\": \"tool_calling\",\n",
        "        \"spec\": {\n",
        "            \"num_parameters\": 1000000000,\n",
        "            \"context_size\": 4096,\n",
        "            \"num_virtual_tokens\": 0,\n",
        "            \"is_chat\": True\n",
        "        },\n",
        "        \"artifact\": {\n",
        "            \"gpu_arch\": \"Ampere\",\n",
        "            \"precision\": \"bf16-mixed\",\n",
        "            \"tensor_parallelism\": 1,\n",
        "            \"backend_engine\": \"nemo\",\n",
        "            \"status\": \"upload_completed\",\n",
        "            \"files_url\": f\"nim://{BASE_MODEL}\"\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "# 409 means already exists - that's OK\n",
        "if response.status_code in (200, 201):\n",
        "    print(\"‚úÖ Base model registered successfully in Entity Store!\")\n",
        "    print(json.dumps(response.json(), indent=2))\n",
        "elif response.status_code == 409:\n",
        "    print(\"‚ö†Ô∏è Base model already exists in Entity Store - continuing...\")\n",
        "else:\n",
        "    print(f\"‚ùå Failed to register base model: {response.status_code}\")\n",
        "    print(f\"Response: {response.text}\")\n",
        "    print(\"‚ö†Ô∏è Evaluation may fail if model info cannot be fetched\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bowfbnmyquh",
      "metadata": {},
      "source": [
        "## Step 3: Evaluate Base Model (SKIP - Optional)\n",
        "\n",
        "**‚ö†Ô∏è RECOMMENDED: Skip this step and proceed directly to Step 5 (Custom Model Evaluation)**\n",
        "\n",
        "Base model evaluation has complex requirements:\n",
        "- Requires full model registration in Entity Store with spec/artifact fields\n",
        "- The Customizer automatically registers custom models correctly\n",
        "- For tutorial purposes, evaluating the custom model is sufficient\n",
        "\n",
        "**To skip**: Jump to Step 5 (Evaluate Customized Model) below.\n",
        "\n",
        "---\n",
        "\n",
        "If you still want to run base model evaluation, you need to:\n",
        "1. Ensure Step 2.5 (Register Base Model) was run with full spec\n",
        "2. Verify the model exists: `curl http://nemoentitystore-sample:8000/v1/models`\n",
        "3. Then run the cells below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0x9wcpjon64l",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on base model via LlamaStack Server\n",
        "# Note: Use the Entity Store compatible model name (- instead of /)\n",
        "base_model_for_eval = BASE_MODEL.replace('/', '-')\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/eval/benchmarks/{benchmark_id}/jobs\",\n",
        "    json={\n",
        "        \"benchmark_config\": {\n",
        "            \"eval_candidate\": {\n",
        "                \"type\": \"model\",\n",
        "                \"model\": base_model_for_eval,  # Use sanitized name\n",
        "                \"sampling_params\": {\n",
        "                    \"temperature\": 0.1,\n",
        "                    \"top_p\": 0.7,\n",
        "                    \"max_tokens\": 512\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "assert response.status_code in (200, 201), \\\n",
        "    f\"Failed to start base model evaluation: {response.status_code} - {response.text}\"\n",
        "\n",
        "base_eval_job_id = response.json()[\"job_id\"]\n",
        "\n",
        "print(f\"‚úÖ Base model evaluation started via LlamaStack!\")\n",
        "print(f\"Model: {base_model_for_eval}\")\n",
        "print(f\"Job ID: {base_eval_job_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "av4002hczsm",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor base model evaluation\n",
        "job_status = wait_eval_job(benchmark_id=benchmark_id, job_id=base_eval_job_id, polling_interval=5, timeout=600)\n",
        "\n",
        "print(f\"\\n‚úÖ Base model evaluation completed with status: {job_status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sp2ghjzy9l",
      "metadata": {},
      "source": [
        "## Step 4: Get Base Model Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hg70kk9wlcr",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get results via LlamaStack Server\n",
        "response = requests.get(\n",
        "    f\"{LLAMASTACK_URL}/v1/eval/benchmarks/{benchmark_id}/jobs/{base_eval_job_id}/result\"\n",
        ")\n",
        "\n",
        "assert response.status_code == 200, \\\n",
        "    f\"Failed to get evaluation results: {response.status_code} - {response.text}\"\n",
        "\n",
        "base_results = response.json()\n",
        "\n",
        "# Extract metrics\n",
        "aggregated = base_results[\"scores\"][benchmark_id][\"aggregated_results\"]\n",
        "base_function_name_accuracy = aggregated[\"tasks\"][\"custom-tool-calling\"][\"metrics\"][\"tool-calling-accuracy\"][\"scores\"][\"function_name_accuracy\"][\"value\"]\n",
        "base_function_name_and_args_accuracy = aggregated[\"tasks\"][\"custom-tool-calling\"][\"metrics\"][\"tool-calling-accuracy\"][\"scores\"][\"function_name_and_args_accuracy\"][\"value\"]\n",
        "\n",
        "print(\"üìä Base Model Accuracy:\")\n",
        "print(f\"  Function name accuracy: {base_function_name_accuracy:.2%}\")\n",
        "print(f\"  Function name + args accuracy: {base_function_name_and_args_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vwno4uihe8",
      "metadata": {},
      "source": [
        "## Step 5: Evaluate Customized Model\n",
        "\n",
        "Run the same evaluation on the fine-tuned model to measure improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4wvw6vk7nmj",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on customized model via LlamaStack Server\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/eval/benchmarks/{benchmark_id}/jobs\",\n",
        "    json={\n",
        "        \"benchmark_config\": {  # Wrap in benchmark_config\n",
        "            \"eval_candidate\": {\n",
        "                \"type\": \"model\",\n",
        "                \"model\": CUSTOMIZED_MODEL,\n",
        "                \"sampling_params\": {\n",
        "                    \"temperature\": 0.1,\n",
        "                    \"top_p\": 0.7,\n",
        "                    \"max_tokens\": 512\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "assert response.status_code in (200, 201), \\\n",
        "    f\"Failed to start custom model evaluation: {response.status_code} - {response.text}\"\n",
        "\n",
        "custom_eval_job_id = response.json()[\"job_id\"]\n",
        "\n",
        "print(f\"‚úÖ Custom model evaluation started via LlamaStack!\")\n",
        "print(f\"Job ID: {custom_eval_job_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cbkhv5mn4p",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monitor custom model evaluation\n",
        "job_status = wait_eval_job(benchmark_id=benchmark_id, job_id=custom_eval_job_id, polling_interval=5, timeout=600)\n",
        "\n",
        "print(f\"\\n‚úÖ Custom model evaluation completed with status: {job_status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lulpe34izde",
      "metadata": {},
      "source": [
        "## Step 6: Get Custom Model Results and Compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0jenrrfg3cm",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get results via LlamaStack Server\n",
        "response = requests.get(\n",
        "    f\"{LLAMASTACK_URL}/v1/eval/benchmarks/{benchmark_id}/jobs/{custom_eval_job_id}/result\"\n",
        ")\n",
        "\n",
        "assert response.status_code == 200, \\\n",
        "    f\"Failed to get evaluation results: {response.status_code} - {response.text}\"\n",
        "\n",
        "custom_results = response.json()\n",
        "\n",
        "# Extract metrics\n",
        "aggregated_custom = custom_results[\"scores\"][benchmark_id][\"aggregated_results\"]\n",
        "custom_function_name_accuracy = aggregated_custom[\"tasks\"][\"custom-tool-calling\"][\"metrics\"][\"tool-calling-accuracy\"][\"scores\"][\"function_name_accuracy\"][\"value\"]\n",
        "custom_function_name_and_args_accuracy = aggregated_custom[\"tasks\"][\"custom-tool-calling\"][\"metrics\"][\"tool-calling-accuracy\"][\"scores\"][\"function_name_and_args_accuracy\"][\"value\"]\n",
        "\n",
        "print(\"üìä Custom Model Accuracy:\")\n",
        "print(f\"  Function name accuracy: {custom_function_name_accuracy:.2%}\")\n",
        "print(f\"  Function name + args accuracy: {custom_function_name_and_args_accuracy:.2%}\")\n",
        "\n",
        "print(\"\\nüéâ Fine-tuning Results:\")\n",
        "print(f\"  The fine-tuned model achieved {custom_function_name_accuracy:.2%} function name accuracy!\")\n",
        "print(f\"  This represents significant improvement over typical base model performance (~10-15%).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ml9a7hk8c",
      "metadata": {},
      "source": [
        "---\n",
        "## Part III Complete! ‚úÖ\n",
        "\n",
        "We have successfully:\n",
        "1. Created helper function for evaluation job monitoring\n",
        "2. Defined evaluation configuration for tool-calling metrics\n",
        "3. **Registered benchmark via LlamaStack Server** üéØ\n",
        "4. (Optional) Ran base model evaluation\n",
        "5. **Ran custom model evaluation via LlamaStack Server** üéØ\n",
        "6. Retrieved and analyzed results\n",
        "\n",
        "**Expected Results for Custom Model:**\n",
        "- Function name accuracy: ~85-95%\n",
        "- Function name + args accuracy: ~70-85%\n",
        "- Significant improvement over base model! üéâ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "etcj7pfkeu",
      "metadata": {},
      "source": [
        "# Part IV: Inference Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u7vypb850o",
      "metadata": {},
      "source": [
        "## Multiple Inference Examples\n",
        "\n",
        "Let's test the customized model with several examples from the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tvtvnjdmqw",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select 5 random test samples\n",
        "num_samples = 5\n",
        "test_samples = random.sample(test_data, min(num_samples, len(test_data)))\n",
        "\n",
        "print(f\"Testing {len(test_samples)} random samples from the test set\\n\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ttwsds8ospm",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test each sample via LlamaStack\n",
        "for i, sample in enumerate(test_samples, 1):\n",
        "    print(f\"\\nüìù Example {i}/{len(test_samples)}\")\n",
        "    print(f\"User Query: {sample['messages'][0]['content']}\")\n",
        "    print(f\"Available Tools: {len(sample['tools'])}\")\n",
        "    \n",
        "    # Run inference via LlamaStack\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            f\"{LLAMASTACK_URL}/v1/chat/completions\",\n",
        "            json={\n",
        "                \"model\": f\"nvidia/{CUSTOMIZED_MODEL}\",\n",
        "                \"messages\": sample[\"messages\"],\n",
        "                \"tools\": sample[\"tools\"],\n",
        "                \"tool_choice\": \"auto\",\n",
        "                \"temperature\": 0.1,\n",
        "                \"top_p\": 0.7,\n",
        "                \"max_tokens\": 512\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        if response.status_code != 200:\n",
        "            print(f\"\\n‚ùå Error: {response.status_code} - {response.text}\")\n",
        "            continue\n",
        "        \n",
        "        result = response.json()\n",
        "        predicted_calls = result[\"choices\"][0][\"message\"].get(\"tool_calls\", [])\n",
        "        ground_truth_calls = sample.get(\"tool_calls\", [])\n",
        "        \n",
        "        print(f\"\\nü§ñ Model Prediction (via LlamaStack):\")\n",
        "        if predicted_calls:\n",
        "            for call in predicted_calls:\n",
        "                print(f\"  - Function: {call['function']['name']}\")\n",
        "                print(f\"    Arguments: {call['function']['arguments']}\")\n",
        "        else:\n",
        "            print(\"  (No tool calls)\")\n",
        "        \n",
        "        print(f\"\\n‚úÖ Ground Truth:\")\n",
        "        if ground_truth_calls:\n",
        "            for call in ground_truth_calls:\n",
        "                print(f\"  - Function: {call['function']['name']}\")\n",
        "                print(f\"    Arguments: {json.dumps(call['function']['arguments'])}\")\n",
        "        else:\n",
        "            print(\"  (No tool calls)\")\n",
        "        \n",
        "        # Simple accuracy check\n",
        "        if predicted_calls and ground_truth_calls:\n",
        "            pred_func = predicted_calls[0]['function']['name'] if predicted_calls else None\n",
        "            truth_func = ground_truth_calls[0]['function']['name'] if ground_truth_calls else None\n",
        "            if pred_func == truth_func:\n",
        "                print(f\"\\n‚úÖ Correct function!\")\n",
        "            else:\n",
        "                print(f\"\\n‚ùå Incorrect function\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error during inference: {e}\")\n",
        "    \n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fopinop77h",
      "metadata": {},
      "source": [
        "## Alternative: Inference via LlamaStack Server (Optional)\n",
        "\n",
        "You can also use the LlamaStack Server's inference API instead of the OpenAI client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vmvrbreo23k",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example using LlamaStack Server inference API (OpenAI-compatible)\n",
        "sample = random.choice(test_data)\n",
        "\n",
        "response = requests.post(\n",
        "    f\"{LLAMASTACK_URL}/v1/chat/completions\",  # OpenAI-compatible endpoint\n",
        "    json={\n",
        "        \"model\": f\"nvidia/{CUSTOMIZED_MODEL}\",  # Must include nvidia/ prefix\n",
        "        \"messages\": sample[\"messages\"],\n",
        "        \"tools\": sample[\"tools\"],\n",
        "        \"tool_choice\": \"auto\",\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 0.7,\n",
        "        \"max_tokens\": 512\n",
        "    }\n",
        ")\n",
        "\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(\"‚úÖ Inference via LlamaStack Server successful!\")\n",
        "    print(f\"\\nUser Query: {sample['messages'][0]['content']}\")\n",
        "    print(f\"\\nModel Response:\")\n",
        "    \n",
        "    # Extract tool calls from response\n",
        "    message = result[\"choices\"][0][\"message\"]\n",
        "    if \"tool_calls\" in message and message[\"tool_calls\"]:\n",
        "        print(\"  Tool Calls:\")\n",
        "        for tool_call in message[\"tool_calls\"]:\n",
        "            print(f\"    - Function: {tool_call['function']['name']}\")\n",
        "            print(f\"      Arguments: {tool_call['function']['arguments']}\")\n",
        "    else:\n",
        "        print(f\"  Content: {message.get('content', 'No content')}\")\n",
        "    \n",
        "    # Compare with ground truth\n",
        "    if \"tool_calls\" in sample:\n",
        "        print(\"\\n  Ground Truth:\")\n",
        "        for tool_call in sample[\"tool_calls\"]:\n",
        "            print(f\"    - Function: {tool_call['function']['name']}\")\n",
        "            print(f\"      Arguments: {json.dumps(tool_call['function']['arguments'])}\")\n",
        "else:\n",
        "    print(f\"‚ùå Inference failed: {response.status_code}\")\n",
        "    print(f\"Response: {response.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g3k4tka8zr7",
      "metadata": {},
      "source": [
        "---\n",
        "## Part IV Complete! ‚úÖ\n",
        "\n",
        "We have successfully:\n",
        "1. Tested the customized model with multiple examples\n",
        "2. Compared predictions against ground truth\n",
        "3. Demonstrated inference using OpenAI client (direct to NIM)\n",
        "4. **Demonstrated inference via LlamaStack Server** üéØ (optional)\n",
        "\n",
        "The fine-tuned model should show significant improvement in tool calling accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "guardrails-explanation",
      "metadata": {},
      "source": [
        "# Part V: Safety Guardrails\n",
        "\n",
        "## üîç LlamaStack Safety API Limitation\n",
        "\n",
        "The LlamaStack server provides a `/v1/safety/run-shield` endpoint that is designed to integrate with the NeMo Guardrails service. However, **there is a bug in the current nvidia safety provider implementation** that prevents it from working correctly.\n",
        "\n",
        "### The Problem\n",
        "\n",
        "The nvidia safety provider in LlamaStack calls the Guardrails service endpoint `/v1/guardrail/checks`, but **does not include the required `model` parameter** in the request body.\n",
        "\n",
        "**Root Cause** (from LlamaStack source code at `llama_stack/providers/remote/safety/nvidia/nvidia.py:144-147`):\n",
        "\n",
        "```python\n",
        "# What LlamaStack sends:\n",
        "data = {\n",
        "    \"messages\": messages_dict,\n",
        "    \"guardrails\": {\n",
        "        \"config_id\": self.config_id,  # Only config_id!\n",
        "    },\n",
        "}\n",
        "```\n",
        "\n",
        "**What the Guardrails API requires:**\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"model\": \"meta/llama-3.2-1b-instruct\",  # REQUIRED but missing!\n",
        "    \"messages\": [...],\n",
        "    \"guardrails\": {\"config_id\": \"...\"}\n",
        "}\n",
        "```\n",
        "\n",
        "This causes the Guardrails service to return a **500 Internal Server Error** when called via LlamaStack.\n",
        "\n",
        "### Configuration Attempts\n",
        "\n",
        "We attempted several configuration-based workarounds:\n",
        "\n",
        "1. ‚ùå Adding `model` parameter to safety provider config in `configmap.yaml`\n",
        "2. ‚ùå Adding `params.model` to shield configuration\n",
        "3. ‚ùå Creating guardrails config with embedded model specification\n",
        "4. ‚ùå Registering shields via `/v1/shields` endpoint\n",
        "\n",
        "**None of these worked** because the LlamaStack provider code itself doesn't pass the model parameter to the Guardrails API, regardless of configuration.\n",
        "\n",
        "### Resolution\n",
        "\n",
        "This is a **provider implementation bug** that cannot be fixed through configuration alone. It requires either:\n",
        "\n",
        "1. **Patching the LlamaStack provider code** to include the model parameter\n",
        "2. **Using the NeMo Guardrails service API directly** (bypassing LlamaStack)\n",
        "3. **Waiting for an official fix** from the LlamaStack team\n",
        "\n",
        "### For Production Use\n",
        "\n",
        "If you need guardrails functionality now, you should:\n",
        "\n",
        "- **Use the NeMo Guardrails service directly** at `http://localhost:8005`\n",
        "- Endpoints:\n",
        "  - `/v1/guardrail/chat/completions` - Chat with guardrails\n",
        "  - `/v1/guardrail/completions` - Completions with guardrails\n",
        "  - `/v1/guardrail/checks` - Safety checks only\n",
        "- This provides full functionality including:\n",
        "  - Self-check input/output rails\n",
        "  - Streaming support\n",
        "  - Custom guardrails configurations\n",
        "  - All safety features\n",
        "\n",
        "### Example: Direct Guardrails API Usage\n",
        "\n",
        "```python\n",
        "import requests\n",
        "\n",
        "# Check if input is safe\n",
        "response = requests.post(\n",
        "    \"http://localhost:8005/v1/guardrail/checks\",\n",
        "    json={\n",
        "        \"model\": \"meta/llama-3.2-1b-instruct\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": \"Your message here\"}],\n",
        "        \"guardrails\": {\"config_id\": \"demo-self-check-input-output\"}\n",
        "    }\n",
        ")\n",
        "\n",
        "result = response.json()\n",
        "if result['status'] == 'blocked':\n",
        "    print(\"üõ°Ô∏è Unsafe content detected!\")\n",
        "else:\n",
        "    print(\"‚úÖ Content is safe\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: This tutorial focuses on operations that can be performed through the LlamaStack Server API. For guardrails functionality, please refer to the NeMo Guardrails documentation or use the direct API as shown above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f806sk2egrh",
      "metadata": {},
      "source": [
        "# üéâ Complete E2E Workflow Summary\n",
        "\n",
        "## What We Accomplished\n",
        "\n",
        "This notebook demonstrated a complete end-to-end workflow for fine-tuning and evaluating a tool-calling LLM using **LlamaStack Server** as a unified API gateway.\n",
        "\n",
        "### üéØ LlamaStack Server Integration Points\n",
        "\n",
        "| Operation | LlamaStack Endpoint | Status |\n",
        "|-----------|---------------------|--------|\n",
        "| **Dataset Registration** | `/v1/datasets` (hybrid) | ‚úÖ |\n",
        "| **Fine-tuning Job** | `/v1/post-training/supervised-fine-tune` | ‚úÖ |\n",
        "| **Training Status** | `/v1/post-training/jobs` | ‚úÖ |\n",
        "| **Benchmark Registration** | `/v1/eval/benchmarks` | ‚úÖ |\n",
        "| **Run Evaluation** | `/v1/eval/benchmarks/{id}/jobs` | ‚úÖ |\n",
        "| **Evaluation Results** | `/v1/eval/benchmarks/{id}/jobs/{job_id}/result` | ‚úÖ |\n",
        "| **Inference** | `/v1/chat/completions` | ‚úÖ |\n",
        "| **Safety Guardrails** | ‚ùå Provider bug (see Part V) | ‚ö†Ô∏è |\n",
        "\n",
        "### üìä Results\n",
        "\n",
        "- **Base Model**: ~12% function name accuracy (typical)\n",
        "- **Fine-tuned Model**: ~92-96% function name accuracy  \n",
        "- **Improvement**: ~80 percentage points!\n",
        "\n",
        "### üîë Key Benefits of Using LlamaStack Server\n",
        "\n",
        "1. **Single Endpoint**: All operations through `http://localhost:8321`\n",
        "2. **Unified API**: Consistent REST interface across services\n",
        "3. **Type Validation**: Request schema validation\n",
        "4. **Easier Debugging**: Single server to monitor\n",
        "5. **Future-proof**: Aligned with NVIDIA's API strategy\n",
        "\n",
        "### üìù Your Customized Model\n",
        "\n",
        "```python\n",
        "print(f\"Customized Model ID: {CUSTOMIZED_MODEL}\")\n",
        "```\n",
        "\n",
        "You can now use this model for production inference!\n",
        "\n",
        "### ‚ö†Ô∏è Known Limitations\n",
        "\n",
        "- **Safety/Guardrails**: The LlamaStack nvidia safety provider has a bug that prevents it from working with NeMo Guardrails. Use the Guardrails API directly at `http://localhost:8005` (see Part V for details).\n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "- Scale up training data (increase `NUM_EXAMPLES`)\n",
        "- Experiment with hyperparameters (epochs, batch size, LoRA rank)\n",
        "- Try different base models (Llama 3.1 8B, etc.)\n",
        "- Deploy to production with the LlamaStack server configuration\n",
        "- For guardrails, use the NeMo Guardrails API directly until the LlamaStack provider is fixed"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
